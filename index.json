[{"content":"","date":"16 February 2024","externalUrl":null,"permalink":"/","section":"Cloudy Caffeine with Ana","summary":"","title":"Cloudy Caffeine with Ana","type":"page"},{"content":"","date":"16 February 2024","externalUrl":null,"permalink":"/coffee/","section":"Coffee","summary":"","title":"Coffee","type":"coffee"},{"content":"","date":"16 February 2024","externalUrl":null,"permalink":"/tags/coffee-reads/","section":"Tags","summary":"","title":"Coffee-Reads","type":"tags"},{"content":"For this coffee read, I wanted to write about the STRIDE model and how, when working in your day to day job, you can use it to break down a functionality or requirement you may have and identify potential security threats.\nThe blog post is more of a guide in how create a security first mindset to approach your work. It\u0026rsquo;s not a guide on how to implement security measures or how to fix the threats (which I will add as part of tech articles). It\u0026rsquo;s more about how to identify them and categorize them.\nOverview of the STRIDE model # The STRIDE model is a framework used to analyze and categorize different types of security threats that can affect computer systems and software applications. Each letter in \u0026ldquo;STRIDE\u0026rdquo; represents a different category of threat:\nSpoofing Identity: This involves attackers impersonating users, systems, or entities to gain unauthorized access or privileges within a system. Spoofing attacks often exploit vulnerabilities in authentication mechanisms.\nTampering with Data: This refers to unauthorized modifications or alterations to data within a system. Tampering attacks can involve changing, deleting, or inserting data to disrupt the integrity or reliability of the information.\nRepudiation: Repudiation threats involve the ability of a user to deny having performed a particular action within a system. This can complicate efforts to trace security incidents and hold individuals accountable for their actions.\nInformation Disclosure: This category covers threats related to the unauthorized exposure or disclosure of sensitive information. Information disclosure attacks can lead to the exposure of confidential data, compromising the privacy and security of individuals or organizations.\nDenial of Service (DoS): Denial of Service attacks aim to disrupt the availability or functionality of a system or network, making it inaccessible to legitimate users. DoS attacks typically overwhelm resources, such as bandwidth or processing power, rendering the system unusable.\nElevation of Privilege: Elevation of privilege threats involve attackers gaining unauthorized access to higher levels of privilege or permissions within a system. This allows attackers to perform actions or access resources that are typically restricted to privileged users.\nImplementing the STRIDE model # Let\u0026rsquo;s say, as part of your work, you are given a requirement or a functionality that is being developed or is going to be implemented and you have to analyze it from a security perspective. You understand the STRIDE model in general, but how would you actually approach breaking down the statement using the STRIDE model especially if you are new to security or if you don\u0026rsquo;t have yet the actual implementation details?\nWhen breaking down a statement using the STRIDE model I followed a few steps that I found helpful:\n1. Identify the Threats # Start by reading the statement carefully and identify any potential security threats or vulnerabilities mentioned. If the requirement concerns a feature about to be implemented think a bit about how that feature will look like and ask yourself questions like: How will the user authenticate? Will it be using a token? How much information will be stored in that token? If someone gets access to that token what can they find about the user? What functionality in your platform will they have access to? Can they manipulate it?\nLook for any actions or scenarios that could lead to unauthorized access, data manipulation, information disclosure, denial of service, or privilege escalation.\nAt this point you want to write down any security concerns that come to mind. Don\u0026rsquo;t worry about categorizing them yet, just write them down. Doesn\u0026rsquo;t matter if they are small or big, if it\u0026rsquo;s a statement or a question. I always think it\u0026rsquo;s better to have an abundance of information that you can later filter out than to miss something important.\nRule of thumb: If you have a concern or question about it, write it down and analyze it the next steps.\n2. Categorize the Threats # Once you\u0026rsquo;ve identified potential threats, try your best to categorize them according to the STRIDE model. Determine which letters in STRIDE are relevant to each threat you\u0026rsquo;ve identified.\nA single threat can be part of multiple categories. For example, a vulnerability in an authentication mechanism could be categorized as a \u0026ldquo;Spoofing Identity\u0026rdquo; threat, while also posing a risk of \u0026ldquo;Elevation of Privilege\u0026rdquo; if exploited. Depending on how you decide to work with the STRIDE model, you can add it to the main category or add it to multiple categories. Whatever makes sense to you and your team should be your guide. The important thing is that everyone has the same understanding on the type of threat and the level of priority that it needs to be handled with, which we will deal with in the next step.\n3. Analyze the Impacts # After you have sifted through all of them, the next thing is to consider the potential impacts or consequences of each threat on the system or application described in the statement or your functionality. Think about how each threat could affect the confidentiality, integrity, availability, and accountability of the system or its users.\nBased on this, you can determine which threats pose the greatest risk to the security of the system and start tackling them first. This will help you focus on the most critical threats while also not disrupting your current workload.\n4. Propose Mitigations # Lastly, propose potential mitigations or countermeasures to address the identified threats and reduce their likelihood or impact. This could involve implementing security controls, patches, or best practices to mitigate vulnerabilities and protect against security threats.\nWhat I\u0026rsquo;d like to highlight here is, when proposing mitigations, it\u0026rsquo;s important to consider the trade-offs. While you focus on security it\u0026rsquo;s a good idea to also keep in mind how the performance or usability of the system will be affected.\nWhen in doubts, it\u0026rsquo;s always a good idea to read up on the best practices of the technology you are using or the type of threat you are dealing with. It\u0026rsquo;s always better to have a good understanding of the threat and the technology you are using before you start implementing any mitigations. Especially if you are just starting out with security.\nExample # Having gone over the thought process of breaking down a statement using the STRIDE model, let\u0026rsquo;s go through an example to see how it can be applied in practice.\nLet\u0026rsquo;s say you have your basic application that allows users to sign in or sign up. With this you also have a Reset Password functionality that allows users to reset their password in case they forget it. Pretty common scenario for most of the platforms we use day to day.\nAnd let\u0026rsquo;s assume we run a test on that endpoint and we find out that it\u0026rsquo;s vulnerable to Cross-Site Request Forgery (CSRF) attacks. Specifically, if the CSRF token is removed from the request, the request is still processed successfully.\nIdentify the Threats # First thing, let\u0026rsquo;s think of the potential threats that could arise from this vulnerability. If the attacker is successful in exploiting this vulnerability, they could potentially gain:\nUnauthorized access to user accounts Unauthorized modification of user passwords Information disclosure of user credentials Potential for account takeover Denial of service due to unauthorized password changes etc. Categorize the Threats # Then, let\u0026rsquo;s categorize these threats according to the STRIDE model:\nSpoofing identity:\nThe attacker could create a CSRF payload, which they host on a malicious website. If users unwittingly trigger this request, it may prompt a password reset action within their authenticated sessions. Consequently, the attacker gains the ability to reset passwords without requiring the user\u0026rsquo;s current password.\nTampering with data:\nThrough the exploitation of CSRF vulnerabilities or open redirect attacks, the attacker can manipulate the password reset process. What this means is they can designate a password of their preference to reset their own account\u0026rsquo;s password, circumventing the necessity for the current user\u0026rsquo;s password.\nRepudiation:\nThe attacker might disown their actions by denying involvement in initiating the password reset requests or by asserting that they had authorization for such actions.\nInformation disclosure:\nSuccessful password reset requests could potentially lead to the exposure of sensitive information like user credentials or personal data if the attacker get access to the user\u0026rsquo;s account.\nDenial of service:\nThese attacks have the potential to result in denial of service by disrupting the normal flow of password reset processes. This could prevent real users to reset their passwords or access their accounts.\nElevation of privilege:\nThe attacker can elevate their privileges by resetting passwords without the need for the current user\u0026rsquo;s password. This enables them to obtain unauthorized entry into user accounts, potentially compromising sensitive data or doing malicious activities on behalf of the user.\nConclusion # To mitigate these threats, you could implement the following apporaches to your work:\nFirst, make sure you have some sort of automated security checks or a security scanner in place. You need to be aware of the vulnerabilities in your system and get fast feedback if new ones are introduced or if the existing ones are indeed fixed (or not).\nThen, you can start by implementing a validation of the CSRF token in the password reset request. For example, validating that the CSRF token in the request body is indeed present and that it is valid. Maybe even add a check that the token is tied to the user\u0026rsquo;s authenticated session information. Just to get you started. Then you can think of more ways to streghten the security like not making the endpoint available to everyone maybe.\nIt\u0026rsquo;s always a good idea to stay up to date on the latest threats whether it is by watching security news and checking if it\u0026rsquo;s applicable to you or checking OWASP TOP 10 or in our example, have a look over CSRF prevention cheat sheet.\nAnd finally, always have a security review before you release a new feature or functionality. It\u0026rsquo;s always better to catch these things before they go live.\nI hope this guide helps you on how to approach breaking down the STRIDE model to your functionality/scenario. If not, I hope it\u0026rsquo;s still a good way to start thinking about security and starting a conversation with your team about it.\nEnjoy your coffee! ☕️\n","date":"16 February 2024","externalUrl":null,"permalink":"/coffee/security-stride/","section":"Coffee","summary":"For this coffee read, I wanted to write about the STRIDE model and how, when working in your day to day job, you can use it to break down a functionality or requirement you may have and identify potential security threats.","title":"Decoding Software Security: A Guide to Assessing Requirements with the STRIDE model","type":"coffee"},{"content":"For this coffee read, I wanted to write about the STRIDE model and how, when working in your day to day job, you can use it to break down a functionality or requirement you may have and identify potential security threats.\nThe blog post is more of a guide in how create a security first mindset to approach your work. It\u0026rsquo;s not a guide on how to implement security measures or how to fix the threats (which I will add as part of tech articles). It\u0026rsquo;s more about how to identify them and categorize them.\nOverview of the STRIDE model # The STRIDE model is a framework used to analyze and categorize different types of security threats that can affect computer systems and software applications. Each letter in \u0026ldquo;STRIDE\u0026rdquo; represents a different category of threat:\nSpoofing Identity: This involves attackers impersonating users, systems, or entities to gain unauthorized access or privileges within a system. Spoofing attacks often exploit vulnerabilities in authentication mechanisms.\nTampering with Data: This refers to unauthorized modifications or alterations to data within a system. Tampering attacks can involve changing, deleting, or inserting data to disrupt the integrity or reliability of the information.\nRepudiation: Repudiation threats involve the ability of a user to deny having performed a particular action within a system. This can complicate efforts to trace security incidents and hold individuals accountable for their actions.\nInformation Disclosure: This category covers threats related to the unauthorized exposure or disclosure of sensitive information. Information disclosure attacks can lead to the exposure of confidential data, compromising the privacy and security of individuals or organizations.\nDenial of Service (DoS): Denial of Service attacks aim to disrupt the availability or functionality of a system or network, making it inaccessible to legitimate users. DoS attacks typically overwhelm resources, such as bandwidth or processing power, rendering the system unusable.\nElevation of Privilege: Elevation of privilege threats involve attackers gaining unauthorized access to higher levels of privilege or permissions within a system. This allows attackers to perform actions or access resources that are typically restricted to privileged users.\nImplementing the STRIDE model # Let\u0026rsquo;s say, as part of your work, you are given a requirement or a functionality that is being developed or is going to be implemented and you have to analyze it from a security perspective. You understand the STRIDE model in general, but how would you actually approach breaking down the statement using the STRIDE model especially if you are new to security or if you don\u0026rsquo;t have yet the actual implementation details?\nWhen breaking down a statement using the STRIDE model I followed a few steps that I found helpful:\n1. Identify the Threats # Start by reading the statement carefully and identify any potential security threats or vulnerabilities mentioned. If the requirement concerns a feature about to be implemented think a bit about how that feature will look like and ask yourself questions like: How will the user authenticate? Will it be using a token? How much information will be stored in that token? If someone gets access to that token what can they find about the user? What functionality in your platform will they have access to? Can they manipulate it?\nLook for any actions or scenarios that could lead to unauthorized access, data manipulation, information disclosure, denial of service, or privilege escalation.\nAt this point you want to write down any security concerns that come to mind. Don\u0026rsquo;t worry about categorizing them yet, just write them down. Doesn\u0026rsquo;t matter if they are small or big, if it\u0026rsquo;s a statement or a question. I always think it\u0026rsquo;s better to have an abundance of information that you can later filter out than to miss something important.\nRule of thumb: If you have a concern or question about it, write it down and analyze it the next steps.\n2. Categorize the Threats # Once you\u0026rsquo;ve identified potential threats, try your best to categorize them according to the STRIDE model. Determine which letters in STRIDE are relevant to each threat you\u0026rsquo;ve identified.\nA single threat can be part of multiple categories. For example, a vulnerability in an authentication mechanism could be categorized as a \u0026ldquo;Spoofing Identity\u0026rdquo; threat, while also posing a risk of \u0026ldquo;Elevation of Privilege\u0026rdquo; if exploited. Depending on how you decide to work with the STRIDE model, you can add it to the main category or add it to multiple categories. Whatever makes sense to you and your team should be your guide. The important thing is that everyone has the same understanding on the type of threat and the level of priority that it needs to be handled with, which we will deal with in the next step.\n3. Analyze the Impacts # After you have sifted through all of them, the next thing is to consider the potential impacts or consequences of each threat on the system or application described in the statement or your functionality. Think about how each threat could affect the confidentiality, integrity, availability, and accountability of the system or its users.\nBased on this, you can determine which threats pose the greatest risk to the security of the system and start tackling them first. This will help you focus on the most critical threats while also not disrupting your current workload.\n4. Propose Mitigations # Lastly, propose potential mitigations or countermeasures to address the identified threats and reduce their likelihood or impact. This could involve implementing security controls, patches, or best practices to mitigate vulnerabilities and protect against security threats.\nWhat I\u0026rsquo;d like to highlight here is, when proposing mitigations, it\u0026rsquo;s important to consider the trade-offs. While you focus on security it\u0026rsquo;s a good idea to also keep in mind how the performance or usability of the system will be affected.\nWhen in doubts, it\u0026rsquo;s always a good idea to read up on the best practices of the technology you are using or the type of threat you are dealing with. It\u0026rsquo;s always better to have a good understanding of the threat and the technology you are using before you start implementing any mitigations. Especially if you are just starting out with security.\nExample # Having gone over the thought process of breaking down a statement using the STRIDE model, let\u0026rsquo;s go through an example to see how it can be applied in practice.\nLet\u0026rsquo;s say you have your basic application that allows users to sign in or sign up. With this you also have a Reset Password functionality that allows users to reset their password in case they forget it. Pretty common scenario for most of the platforms we use day to day.\nAnd let\u0026rsquo;s assume we run a test on that endpoint and we find out that it\u0026rsquo;s vulnerable to Cross-Site Request Forgery (CSRF) attacks. Specifically, if the CSRF token is removed from the request, the request is still processed successfully.\nIdentify the Threats # First thing, let\u0026rsquo;s think of the potential threats that could arise from this vulnerability. If the attacker is successful in exploiting this vulnerability, they could potentially gain:\nUnauthorized access to user accounts Unauthorized modification of user passwords Information disclosure of user credentials Potential for account takeover Denial of service due to unauthorized password changes etc. Categorize the Threats # Then, let\u0026rsquo;s categorize these threats according to the STRIDE model:\nSpoofing identity:\nThe attacker could create a CSRF payload, which they host on a malicious website. If users unwittingly trigger this request, it may prompt a password reset action within their authenticated sessions. Consequently, the attacker gains the ability to reset passwords without requiring the user\u0026rsquo;s current password.\nTampering with data:\nThrough the exploitation of CSRF vulnerabilities or open redirect attacks, the attacker can manipulate the password reset process. What this means is they can designate a password of their preference to reset their own account\u0026rsquo;s password, circumventing the necessity for the current user\u0026rsquo;s password.\nRepudiation:\nThe attacker might disown their actions by denying involvement in initiating the password reset requests or by asserting that they had authorization for such actions.\nInformation disclosure:\nSuccessful password reset requests could potentially lead to the exposure of sensitive information like user credentials or personal data if the attacker get access to the user\u0026rsquo;s account.\nDenial of service:\nThese attacks have the potential to result in denial of service by disrupting the normal flow of password reset processes. This could prevent real users to reset their passwords or access their accounts.\nElevation of privilege:\nThe attacker can elevate their privileges by resetting passwords without the need for the current user\u0026rsquo;s password. This enables them to obtain unauthorized entry into user accounts, potentially compromising sensitive data or doing malicious activities on behalf of the user.\nConclusion # To mitigate these threats, you could implement the following apporaches to your work:\nFirst, make sure you have some sort of automated security checks or a security scanner in place. You need to be aware of the vulnerabilities in your system and get fast feedback if new ones are introduced or if the existing ones are indeed fixed (or not).\nThen, you can start by implementing a validation of the CSRF token in the password reset request. For example, validating that the CSRF token in the request body is indeed present and that it is valid. Maybe even add a check that the token is tied to the user\u0026rsquo;s authenticated session information. Just to get you started. Then you can think of more ways to streghten the security like not making the endpoint available to everyone maybe.\nIt\u0026rsquo;s always a good idea to stay up to date on the latest threats whether it is by watching security news and checking if it\u0026rsquo;s applicable to you or checking OWASP TOP 10 or in our example, have a look over CSRF prevention cheat sheet.\nAnd finally, always have a security review before you release a new feature or functionality. It\u0026rsquo;s always better to catch these things before they go live.\nI hope this guide helps you on how to approach breaking down the STRIDE model to your functionality/scenario. If not, I hope it\u0026rsquo;s still a good way to start thinking about security and starting a conversation with your team about it.\nEnjoy your coffee! ☕️\n","date":"16 February 2024","externalUrl":null,"permalink":"/reads/security-stride/","section":"Reads","summary":"For this coffee read, I wanted to write about the STRIDE model and how, when working in your day to day job, you can use it to break down a functionality or requirement you may have and identify potential security threats.","title":"Decoding Software Security: A Guide to Assessing Requirements with the STRIDE model","type":"reads"},{"content":"","date":"16 February 2024","externalUrl":null,"permalink":"/reads/","section":"Reads","summary":"","title":"Reads","type":"reads"},{"content":"","date":"16 February 2024","externalUrl":null,"permalink":"/tags/security/","section":"Tags","summary":"","title":"Security","type":"tags"},{"content":"","date":"16 February 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"7 February 2024","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"Aws","type":"tags"},{"content":"This blog post will be a quick one focusing on troubleshooting a less clear error, \u0026lsquo;Cannot delete entity, must remove tokens from principal first\u0026rsquo;, that Terraform can throw when you try to delete IAM users from AWS.\nLet\u0026rsquo;s assume that in your Terraform configuration you manage IAM users and you want to delete one of them. You\u0026rsquo;d think that by simply removing the Terraform code and then running terraform apply it will delete the users. Which was my case. But then as soon as I ran the command to destroy the resource I ran into an issue:\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # aws_iam_user.little_tester will be destroyed # (because aws_iam_user.little_tester is not in configuration) - resource \u0026#34;aws_iam_user\u0026#34; \u0026#34;little_tester\u0026#34; { - arn = \u0026#34;arn:aws:iam::xxxxxxxxxx:user/little_tester\u0026#34; -\u0026gt; null - force_destroy = false -\u0026gt; null - id = \u0026#34;little_tester\u0026#34; -\u0026gt; null - name = \u0026#34;little_tester\u0026#34; -\u0026gt; null - path = \u0026#34;/\u0026#34; -\u0026gt; null - tags = { - \u0026#34;Company\u0026#34; = \u0026#34;MyCompany\u0026#34; - \u0026#34;Location\u0026#34; = \u0026#34;Aruba\u0026#34; - \u0026#34;Unit\u0026#34; = \u0026#34;Front Desk\u0026#34; } -\u0026gt; null - tags_all = { - \u0026#34;Company\u0026#34; = \u0026#34;MyCompany\u0026#34; - \u0026#34;Location\u0026#34; = \u0026#34;Aruba\u0026#34; - \u0026#34;Unit\u0026#34; = \u0026#34;Front Desk\u0026#34; } -\u0026gt; null - unique_id = \u0026#34;AAAAAAAAAAAAAAAAA\u0026#34; -\u0026gt; null } Plan: 0 to add, 0 to change, 1 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes aws_iam_user.little_tester: Destroying... [id=little_tester] ╷ │ Error: deleting IAM User (little_tester): DeleteConflict: Cannot delete entity, must remove tokens from principal first. │ status code: 409, request id: ... │ So what does this mean? # The error Cannot delete entity, must remove tokens from principal first. says that the user has some tokens that need to be removed before the user itself can be deleted. The tokens it refers to can be active access keys or registered MFA devices.\nThe decision to prevent the deletion of a user if any of these active tokens are associated to it makes sense since from a security perspective because it aims to prevent accidental deletion of users that are still active.\nA way to confirm if this is the case is to go to AWS Console and check the user\u0026rsquo;s Security credentials. There you should see any active access keys or registered MFA devices.\nHaving checked that, I saw that the user had an Access key that was still active and had an active MFA device. I removed both manually and then ran terraform apply again. And it worked! The user was deleted successfully.\nHow can this happen? # The user\u0026rsquo;s access token and the MFA device configured to his account were not managed by Terraform, meaning they were created manually. So Terraform was not aware of them and could not delete them. And this was preventing the deletion of the user.\nHow this could come to be is if the user was created through Terraform code, but all the other configurations were done manually after the user was created: adding an access key, adding an MFA device, etc. So then you end up with a mix of Terraform-managed and non-Terraform-managed resources.\nSomething to think about for future cases, this could also happen if you create a user group in Terraform and then add users to it manually later on. These users will be part of the group, but Terraform will not be aware of them and will not be able to manage them. Or in any other scenario where you mix non-Terraform-managed and Terraform-managed resources.\nWhat can you do? # First option, is to add the access key and MFA device to the Terraform configuration so then creation and removal of the users will be part of a complete flow fully managed by Terraform.\nSecond option is to simply manually go to AWS Console \u0026gt; IAM, and check the user\u0026rsquo;s Security credentials and MFA devices. For the active ones simply deactivate them and remove them manually. Then simply run to your configuration and run terraform apply again.\nAnd lastly, you can add the force_destroy argument to the aws_iam_user resource in your Terraform configuration.\nforce_destroy - (Optional, default false) When destroying this user, destroy even if it has non-Terraform-managed IAM access keys, login profile or MFA devices. Without force_destroy a user with non-Terraform-managed access keys and login profile will fail to be destroyed.\nBy enabling it, it will allow Terraform to delete the user even if it has non-Terraform-managed access keys and MFA devices.\nWarning!\nWhile it does seem a convenient option, be very careful with this argument, as it can lead to accidental deletion of users that are still active. So I would advise you to use it only if you are sure that the user is not active (maybe have a check in place that runs before the destruction of the resources), that you are aware of the security implications and lastly check the access of the team members that can run the Terraform code.\nConclusion # If Terraform cannot delete your AWS IAM users remember to check the user\u0026rsquo;s Security credentials and look for any active access keys or MFA devices. If they are active, deactivate and remove them. How you handle it in your Terraform code is up to you, but remember to be careful with the force_destroy option.\nHope this helps someone out there!\n","date":"7 February 2024","externalUrl":null,"permalink":"/posts/aws-iam-deleting-users/","section":"Posts","summary":"This blog post will be a quick one focusing on troubleshooting a less clear error, \u0026lsquo;Cannot delete entity, must remove tokens from principal first\u0026rsquo;, that Terraform can throw when you try to delete IAM users from AWS.","title":"AWS: Handling 'Cannot delete entity, must remove tokens from principal first' error","type":"posts"},{"content":"","date":"7 February 2024","externalUrl":null,"permalink":"/tags/iam/","section":"Tags","summary":"","title":"Iam","type":"tags"},{"content":"","date":"7 February 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"7 February 2024","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"","date":"31 October 2023","externalUrl":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure","type":"tags"},{"content":"Recently, I had to enable WAF on our Azure Application Gateway. Because of our infrastructure setup, I wanted to have all the rules from OWASP 3.2 enabled, but I needed to be able to exclude some of our (valid) requests from being blocked as well. To achieve this, I could either try to configure the WAF Config section on our Gateway or create a WAF policy.\nGiven that it was not entirely clear how you can use proper exclusions and filters based on what you need, I decided to write this post to explain the differences I found between the two and how you can use them.\nWhat is WAF? # To recap what Web Application Firewall (WAF) is, here is a brief explanation from the official documentation:\nAzure Web Application Firewall (WAF) on Azure Application Gateway provides centralized protection of your web applications from common exploits and vulnerabilities. Web applications are increasingly targeted by malicious attacks that exploit commonly known vulnerabilities. SQL injection and cross-site scripting are among the most common attacks.\nWAF on Application Gateway is based on the Core Rule Set (CRS) from the Open Web Application Security Project (OWASP).\nBefore being able to enable and benefit from WAF capabilities, you will need to check the SKU of the Application Gateway you have. WAF can only be enabled on the WAF_v2 SKU and not the Standard SKU. As this was my case as well, I first had to change the SKU of the Application Gateway. This can be done either from the Azure Portal or using Terraform (or any other tool for IaC, in my case, I used this one).\nAfter this, you can proceed with configuring WAF. This can be done in two ways: either using the built-in WAF Config section of the Application Gateway or creating a WAF policy for the Azure Application Gateway.\nLet\u0026rsquo;s look at what each one is and how you can use them.\nWAF config # The WAF config section is a built-in part of the Application Gateway configuration as can be seen in the image below:\nWAF Config is the Application Gateway\u0026rsquo;s built-in method to configure WAF and it is the section where you can add your configurations such as exclusions or custom rules.\nWhen using Terraform you can find the waf_configuration block under the azurerm_application_gateway resource.\nLet\u0026rsquo;s look at an example of how you can configure it using Terraform.\nScenario: I would like to configure it to use OWASP 3.2 rules, enable the WAF, and exclude some of our telemetry requests from being blocked while also disabling some rules. This is how the basic configuration would look like:\nresource \u0026#34;azurerm_application_gateway\u0026#34; \u0026#34;application_gateway\u0026#34; { (...) \u0026#34;waf_configuration\u0026#34; { enabled = true firewall_mode = \u0026#34;Prevention\u0026#34; rule_set_type = \u0026#34;OWASP\u0026#34; rule_set_version = \u0026#34;3.2\u0026#34; file_upload_limit_mb = 100 max_request_body_size_kb = 128 request_body_check = false exclusion { match_variable = \u0026#34;RequestCookieNames\u0026#34; selector = \u0026#34;telemetry\u0026#34; selector_match_operator = \u0026#34;Contains\u0026#34; } disabled_rule_group { rule_group_name = \u0026#34;REQUEST-920-PROTOCOL-ENFORCEMENT\u0026#34; rules = [ 920230, 920320, ] } disabled_rule_group { rule_group_name = \u0026#34;REQUEST-921-PROTOCOL-ATTACK\u0026#34; rules = [ 921180, 921170, ] } } } The scenario was simple and while the configuration itself is not hard to do, there are a few drawbacks to using it:\nit does not allow you to add custom rules from the Azure Portal UI. This means that if you want to add a custom rule, you will have to do it using the Azure CLI (or PowerShell). I would like to ideally have all my configurations in one place and not have to use multiple tools to configure or maintain my resources.\nif you have multiple Application Gateways, you will have to configure each one of them separately. Because WAF Config is built-in the Application Gateway this also means it is managed locally to that specific Application Gateway. While it\u0026rsquo;s configuration applies to everything in the Azure Application Gateway resource. Which was my case as well as I don\u0026rsquo;t manage just one Application Gateway.\nif you are working with Azure Front Door it\u0026rsquo;s good to know that you cannot use WAF Config in that context. This is because Azure Front Door does not support WAF Config.\nWAF policy # As opposed to WAF Config, which is a built-in functionality in the Application Gateway, WAF policies are a standalone resource that enables you to configure WAF. This means that you can create a WAF policy and then apply it to multiple Application Gateways or even Azure Front Door resources as well.\nWAF policy allows you to have a centralized configuration for all your WAF resources. This means that you can have the same configuration for all your WAF resources and you can also have a single place where you can manage your WAF configuration.\nBecause it is a standalone resource the first benefit is you will be able to find all the configurations necessary in the Azure Portal UI: TODO: Rephrase\nYou have your Managed rules:\nYour Custom rules:\nAnd your associated gateways:\nAs you can already guess from the screenshots, WAF Policy gives you a bit more control over your configuration as you can be more detailed in what you want to exclude or include in your rules.\nYou have the flexibility to link a WAF (Web Application Firewall) policy in various ways: you can connect it:\nglobally by assigning it to an Azure Application Gateway resource per-site level by linking it to a listener per URI level by associating it with a particular route path For more details (and examples) on how you can link a WAF policy to your resources, you can check the official documentation here.\nIn Terraform this means you will need to create a new resource:\nresource \u0026#34;azurerm_web_application_firewall_policy\u0026#34; \u0026#34;waf_policy\u0026#34; { name = \u0026#34;wafpolicy\u0026#34; resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location policy_settings { enabled = true mode = \u0026#34;Prevention\u0026#34; request_body_check = false file_upload_limit_in_mb = 100 max_request_body_size_in_kb = 128 } ## Example of managed rules managed_rules { managed_rule_set { type = \u0026#34;OWASP\u0026#34; version = \u0026#34;3.2\u0026#34; rule_group_override { rule_group_name = \u0026#34;REQUEST-920-PROTOCOL-ENFORCEMENT\u0026#34; disabled_rules = [ 920200, 920201, 920202 ] } rule_group_override { rule_group_name = \u0026#34;REQUEST-921-PROTOCOL-ATTACK\u0026#34; disabled_rules = [ 921170, 921180, ] } rule_group_override { rule_group_name = \u0026#34;REQUEST-942-APPLICATION-ATTACK-SQLI\u0026#34; disabled_rules = [ 942430 ] } } } ## Example of custom rules custom_rules { name = \u0026#34;ExcludeServicesFromWAF\u0026#34; priority = 14 rule_type = \u0026#34;MatchRule\u0026#34; match_conditions { match_variables { variable_name = \u0026#34;RequestUri\u0026#34; } operator = \u0026#34;Contains\u0026#34; negation_condition = false match_values = [ \u0026#34;/service1/\u0026#34;, (...) ] } action = \u0026#34;Allow\u0026#34; } } After creating the WAF Policy you will need to associate it to the Application Gateway which will be done by adding the following parameters to the azurerm_application_gateway resource:\nfirewall_policy_id = azurerm_web_application_firewall_policy.wafpolicy.id force_firewall_policy_association = true The first thing you will notice, if you go to Azure Portal, is that in your Application Gatway resource you will no longer have the WAF Config section available, but a link to the WAF Policy you just created:\nThis means that any change you want to make to your WAF configuration you will need to do it in the WAF Policy resource itself and not in the Application Gateway resource. This offered me the granularity I needed to be able to exclude the requests I wanted and also have the same configuration for all my Application Gateways.\nIn my case, WAF Config was not the right answer for what I needed: have the same exclusions on all our gateways and also have the same custom rules regradless of environment and allow me to exclude the requests that were coming from our services. This is why I decided to look into WAF policies instead.\nFinal thoughts # WAF Config is a good option if you want to configure WAF settings at Application Gateway level that applies to all the listeners and rules within it. It\u0026rsquo;s quite suitable if you have a single set of WAF settings that you want to apply to all your web applications behind the Application Gateway.\nWhereas, WAF Policy will be a good choice when you need to have a more granular control over your WAF settings, where you need to define custom WAF settings and rules per-application or per-bath basis. One use-case for this could be if you have several applications behind the Application Gateway that have different security concerns and require configuring different WAF settings.\nI did not dive into all the rules and settings you can configure for WAF, which will be the topic of a separate more in-depth article, but I hope this post will help you decide which one is the best option for you.\nThank you for reading and hope this helps somebody else!\n","date":"31 October 2023","externalUrl":null,"permalink":"/posts/azure-waf-configurations/","section":"Posts","summary":"Recently, I had to enable WAF on our Azure Application Gateway.","title":"Azure Application Gateway WAF config vs WAF policy","type":"posts"},{"content":" For this coffee read, I will be reviewing the book Software Architecture Patterns (Report) by Mark Richards.\nI want to start with a disclaimer on this one. I have not worked as a software architect. So this book review will be from the perspective of someone that has worked as a DevOps engineer (more of a Cloud Engineer if we have to label it), and that has been involved in the design of systems and the architecture of systems. My personal belief is you need to have a good high-level view of the system you are working on, regardless of the project or your skillset (or job title), in order to be able to design or test solutions that make it scalable, reliable, and maintainable.\nMy hopes, from reading this book, were to get a better understanding of the different architecture patterns that are out there, how they can be used to design systems, what are some things to look out for when choosing one or the other or a combination of them and what would be some good questions to ask yourself when opting between them.\nSoftware Architecture Patterns (Report) # The book, in its own description, emphasizes the importance of having a clear and well-defined architecture in software development. Without it, developers tend to default to traditional layered patterns, which can result in disorganized code known as the \u0026ldquo;big ball of mud\u0026rdquo; anti-pattern. Applications lacking a formal architecture are typically tightly coupled, difficult to change, and lack clarity in terms of their characteristics.\nHaving a defined architecture pattern helps establish the fundamental characteristics and behavior of an application. Different architecture patterns are suitable for different needs, such as scalability or agility. Understanding the strengths and weaknesses of each pattern is essential to choosing the one that aligns with specific business requirements.\nThe goal of the book, as mentioned by the author, is to provide architects with the information needed to make and justify these decisions effectively (hence my disclaimer).\nThoughts on the book # The book is broken down into the following chapters:\nLayered Architecture Event-Driven Architecture Microkernel Architecture Microservices Architecture Space-Based Architecture Each type is nicely broken down into sections that explain the pattern description, key concepts, pattern examples, consideration, and pattern analysis.\nThe book also provides a summary of the different patterns in terms of their characteristics, strengths, weaknesses, and when to use them in real-life scenarios.\nEach chapter starts with a summary of what the architectural pattern is and what are the key concepts behind it. It then goes into detail about the pattern (including design samples) and how it can be used by providing examples of real-life scenarios. For example, for the Event-Driven Architecture pattern, the author explains the mediator and broker topologies and how they can be used to implement the pattern before delving into the details of the pattern itself.\nNext, it highlights the considerations that need to be taken into account when choosing that particular architecture. For example, in the Event-Driven Architecture pattern, the author explains that one of the challenges of this architecture pattern is the lack of atomic transactions for a single business process. And this is the reason why when using this pattern you need to continuously think about which events can and cannot run independently and plan the granularity of your event processors accordingly.\nFinally, it provides an analysis of the pattern in terms of Overall agility, Ease of deployment, Testability, Performance, Scalability, and Ease of development. The author gives a rating from Low to High on each of these attributes when considering the architecture pattern and an analysis as to why it scored that way. This really made me understand why one pattern would be better than another in a specific scenario. Or at least what issues you can run into when choosing it. For example, Microservices pattern is rated as High in terms of Overall agility and Ease of deployment but Low in terms of Performance. This is because of the distributed nature of the pattern, even though you can most definitely create applications that are performant using this pattern.\nIt was easy to understand the concepts and follow the examples, and the analysis of the patterns was very useful in understanding the strengths and weaknesses of each pattern.\nThis analysis also guided me in understanding the trade-offs associated with choosing one pattern over another. It provided valuable insights into the kinds of questions that should be considered when making such a decision, whether individually or within a team context. Moreover, it encouraged a shift in my perspective from seeking the \u0026lsquo;perfect\u0026rsquo; pattern to identifying the pattern that aligns best with the specific requirements of the system.\nAs a result, this change in mindset aided me in asking pertinent questions when designing a system, with a strong focus on the system\u0026rsquo;s characteristics and the demands of the business, ultimately contributing to more informed and effective decision-making.\nSummary # This book was a very good read as it achieved what it set out to do.\nI would recommend it as a very good starting point for anyone interested in learning more about different software architecture patterns. And you have the option of diving deeper into the patterns that are of interest to them in other books from this authos or others.\nIf you\u0026rsquo;ve read the book, let me know what you thought about it in the comments below. If you have any recommendations for other books on the topic of software architecture, I would love to hear them.\nEnjoy your coffee! ☕️\n","date":"4 October 2023","externalUrl":null,"permalink":"/coffee/book-review-sw-architecture-patterns-report/","section":"Coffee","summary":"For this coffee read, I will be reviewing the book Software Architecture Patterns (Report) by Mark Richards.","title":"Book Review: Software Architecture Patterns (Report)","type":"coffee"},{"content":" For this coffee read, I will be reviewing the book Software Architecture Patterns (Report) by Mark Richards.\nI want to start with a disclaimer on this one. I have not worked as a software architect. So this book review will be from the perspective of someone that has worked as a DevOps engineer (more of a Cloud Engineer if we have to label it), and that has been involved in the design of systems and the architecture of systems. My personal belief is you need to have a good high-level view of the system you are working on, regardless of the project or your skillset (or job title), in order to be able to design or test solutions that make it scalable, reliable, and maintainable.\nMy hopes, from reading this book, were to get a better understanding of the different architecture patterns that are out there, how they can be used to design systems, what are some things to look out for when choosing one or the other or a combination of them and what would be some good questions to ask yourself when opting between them.\nSoftware Architecture Patterns (Report) # The book, in its own description, emphasizes the importance of having a clear and well-defined architecture in software development. Without it, developers tend to default to traditional layered patterns, which can result in disorganized code known as the \u0026ldquo;big ball of mud\u0026rdquo; anti-pattern. Applications lacking a formal architecture are typically tightly coupled, difficult to change, and lack clarity in terms of their characteristics.\nHaving a defined architecture pattern helps establish the fundamental characteristics and behavior of an application. Different architecture patterns are suitable for different needs, such as scalability or agility. Understanding the strengths and weaknesses of each pattern is essential to choosing the one that aligns with specific business requirements.\nThe goal of the book, as mentioned by the author, is to provide architects with the information needed to make and justify these decisions effectively (hence my disclaimer).\nThoughts on the book # The book is broken down into the following chapters:\nLayered Architecture Event-Driven Architecture Microkernel Architecture Microservices Architecture Space-Based Architecture Each type is nicely broken down into sections that explain the pattern description, key concepts, pattern examples, consideration, and pattern analysis.\nThe book also provides a summary of the different patterns in terms of their characteristics, strengths, weaknesses, and when to use them in real-life scenarios.\nEach chapter starts with a summary of what the architectural pattern is and what are the key concepts behind it. It then goes into detail about the pattern (including design samples) and how it can be used by providing examples of real-life scenarios. For example, for the Event-Driven Architecture pattern, the author explains the mediator and broker topologies and how they can be used to implement the pattern before delving into the details of the pattern itself.\nNext, it highlights the considerations that need to be taken into account when choosing that particular architecture. For example, in the Event-Driven Architecture pattern, the author explains that one of the challenges of this architecture pattern is the lack of atomic transactions for a single business process. And this is the reason why when using this pattern you need to continuously think about which events can and cannot run independently and plan the granularity of your event processors accordingly.\nFinally, it provides an analysis of the pattern in terms of Overall agility, Ease of deployment, Testability, Performance, Scalability, and Ease of development. The author gives a rating from Low to High on each of these attributes when considering the architecture pattern and an analysis as to why it scored that way. This really made me understand why one pattern would be better than another in a specific scenario. Or at least what issues you can run into when choosing it. For example, Microservices pattern is rated as High in terms of Overall agility and Ease of deployment but Low in terms of Performance. This is because of the distributed nature of the pattern, even though you can most definitely create applications that are performant using this pattern.\nIt was easy to understand the concepts and follow the examples, and the analysis of the patterns was very useful in understanding the strengths and weaknesses of each pattern.\nThis analysis also guided me in understanding the trade-offs associated with choosing one pattern over another. It provided valuable insights into the kinds of questions that should be considered when making such a decision, whether individually or within a team context. Moreover, it encouraged a shift in my perspective from seeking the \u0026lsquo;perfect\u0026rsquo; pattern to identifying the pattern that aligns best with the specific requirements of the system.\nAs a result, this change in mindset aided me in asking pertinent questions when designing a system, with a strong focus on the system\u0026rsquo;s characteristics and the demands of the business, ultimately contributing to more informed and effective decision-making.\nSummary # This book was a very good read as it achieved what it set out to do.\nI would recommend it as a very good starting point for anyone interested in learning more about different software architecture patterns. And you have the option of diving deeper into the patterns that are of interest to them in other books from this authos or others.\nIf you\u0026rsquo;ve read the book, let me know what you thought about it in the comments below. If you have any recommendations for other books on the topic of software architecture, I would love to hear them.\nEnjoy your coffee! ☕️\n","date":"4 October 2023","externalUrl":null,"permalink":"/reads/book-review-sw-architecture-patterns-report/","section":"Reads","summary":"For this coffee read, I will be reviewing the book Software Architecture Patterns (Report) by Mark Richards.","title":"Book Review: Software Architecture Patterns (Report)","type":"reads"},{"content":"","date":"4 October 2023","externalUrl":null,"permalink":"/tags/book-review/","section":"Tags","summary":"","title":"Book-Review","type":"tags"},{"content":"","date":"18 July 2023","externalUrl":null,"permalink":"/tags/dapr/","section":"Tags","summary":"","title":"Dapr","type":"tags"},{"content":"A CNCF project, the Distributed Application Runtime (Dapr) provides APIs that simplify microservice connectivity. Whether your communication pattern is service to service invocation or pub/sub messaging, Dapr helps you write resilient and secured microservices. Essentially, it provides a new way to build microservices by using the reusable blocks implemented as sidecars.\nWhile Dapr is great as it is language agnostic and it solves some challenges that come with microservices and distributed systems, such as message broker integration, encryption etc, troubleshooting Dapr issues can be quite challenging. Dapr logs, especially the error messages, can be quite generic and sometimes do not provide enough information for you to understand what is going on.\nIn this blog post, I want to detail a problem I had with Dapr certificate expiration, how I troubleshoot the root cause, the symptoms the application was manifesting and how I managed to solve it.\nI want also to highlight how important it is to have proper monitoring in place so I will be touching upon that as well by showing you some lessons learned and what I ended setting up to save me from repeating the same mistakes in the future.\nSymptoms # Application deployment was failing because it could not inject the dapr sidecar. It kept restarting until reaching the 5min defaut timeout and rolledback. Checking the events on the pod I noticed the GET /healthz endpoints for liveness and readiness probes were throwing connect: connection refused.\nThere were no errors in app logs or in the Dapr sidecar logs. The only thing I noticed was that the dapr sidecar was in CrashLoopBackOff state.\nTroubleshooting # Step 1: Dapr Operator logs # Since no longs were available on pod or Dapr sidecar, I started by checking the logs of the next best thing which was the Dapr Operator and I noticed the following error:\n{\u0026#34;instance\u0026#34;:\u0026#34;dapr-operator-0000000000-abcd\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;starting webhooks\u0026#34;,\u0026#34;scope\u0026#34;:\u0026#34;dapr.operator\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-05-25T12:51:13.267369255Z\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;log\u0026#34;,\u0026#34;ver\u0026#34;:\u0026#34;1.10.4\u0026#34;} I0525 12:51:13.269285 1 leaderelection.go:248] attempting to acquire leader lease dapr-system/webhooks.dapr.io... {\u0026#34;instance\u0026#34;:\u0026#34;dapr-operator-0000000000-abcd\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Conversion webhook for \\\u0026#34;subscriptions.dapr.io\\\u0026#34; is up to date\u0026#34;,\u0026#34;scope\u0026#34;:\u0026#34;dapr.operator\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-05-25T12:51:13.277615379Z\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;log\u0026#34;,\u0026#34;ver\u0026#34;:\u0026#34;1.10.4\u0026#34;} W0601 02:52:46.530879 1 reflector.go:347] pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Secret ended with: an error on the server (\u0026#34;unable to decode an event from the watch stream: http2: client connection lost\u0026#34;) has prevented the request from succeeding W0601 02:52:46.531001 1 reflector.go:347] pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1alpha1.Configuration ended with: an error on the server (\u0026#34;unable to decode an event from the watch stream: http2: client connection lost\u0026#34;) has prevented the request from succeeding W0601 02:52:46.531061 1 reflector.go:347] pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Service ended with: an error on the server (\u0026#34;unable to decode an event from the watch stream: http2: client connection lost\u0026#34;) has prevented the request from succeeding E0601 02:52:46.531050 1 leaderelection.go:330] error retrieving resource lock dapr-system/operator.dapr.io: Get \u0026#34;https://X.X.X.X:443/apis/coordination.k8s.io/v1/namespaces/dapr-system/leases/operator.dapr.io\u0026#34;: http2: client connection lost W0601 02:52:46.531095 1 reflector.go:347] pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1alpha1.Resiliency ended with: an error on the server (\u0026#34;unable to decode an event from the watch stream: http2: client connection lost\u0026#34;) has prevented the request from succeeding W0601 02:52:46.530891 1 reflector.go:347] pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1alpha1.Component ended with: an error on the server (\u0026#34;unable to decode an event from the watch stream: http2: client connection lost\u0026#34;) has prevented the request from succeeding E0601 02:52:46.531191 1 leaderelection.go:330] error retrieving resource lock dapr-system/webhooks.dapr.io: Get \u0026#34;https://X.X.X.X:443/apis/coordination.k8s.io/v1/namespaces/dapr-system/leases/webhooks.dapr.io\u0026#34;: http2: client connection lost The Dapr operator works by establishing an admission webhook, which enables Kubernetes (K8s) to interact with it when it intends to deploy a new pod. After a successful response, the daprd container is added to the pod. For more in detail information on how the operator works, check the Dapr Operator control plane service overview documentation.\nStep 2: Investigate the http2: client connection lost error # The http2: client connection lost error indicated to me that K8s could not successfully invoke the admission webhook, so I started to check one by one:\nNetwork connectivity: The error message mentioned a potential issue with the client connection being lost. So I verified that the machine running the Dapr process could establish a stable connection to the Kubernetes API server. Checked for any network connectivity issues or firewalls that might be interfering with the communication. Everything was fine.\nAPI server issues: I also checked for any issues with the Kubernetes API server itself, such as high load, resource constraints, or misconfiguration. No issues found.\nNamespace or resource deletion: I checked that no resources had been deleted in the the dapr-system namespace or the webhooks.dapr.io resource. Everything was still there.\nStep 3: AKS cluster logs # So as next step, I started looking into the AKS cluster logs and noticed that all the services that were also using Dapr had the following error authentication handshake failed. The full log is below:\n{\u0026#34;app_id\u0026#34;:\u0026#34;app1\u0026#34;,\u0026#34;instance\u0026#34;:\u0026#34;app1-123456-abc7\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;sending workload csr request to sentry\u0026#34;,\u0026#34;scope\u0026#34;:\u0026#34;dapr.runtime.grpc.internal\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-06-19T13:19:53.535345802Z\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;log\u0026#34;,\u0026#34;ver\u0026#34;:\u0026#34;1.10.4\u0026#34;} 2023-06-19 15:19:53.535\t{\u0026#34;app_id\u0026#34;:\u0026#34;app1\u0026#34;,\u0026#34;instance\u0026#34;:\u0026#34;app1-123456-abc7\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;renewing certificate: requesting new cert and restarting gRPC server\u0026#34;,\u0026#34;scope\u0026#34;:\u0026#34;dapr.runtime.grpc.internal\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-06-19T13:19:53.535329702Z\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;log\u0026#34;,\u0026#34;ver\u0026#34;:\u0026#34;1.10.4\u0026#34;} 2023-06-19 15:19:53.535\t{\u0026#34;app_id\u0026#34;:\u0026#34;app1\u0026#34;,\u0026#34;instance\u0026#34;:\u0026#34;app1-123456-abc7\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;error starting server: error from authenticator CreateSignedWorkloadCert: error from sentry SignCertificate: rpc error: code = Unavailable desc = connection error: desc = \\\u0026#34;transport: authentication handshake failed: tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2023-06-19T13:19:51Z is after 2023-06-16T12:31:17Z\\\u0026#34;\u0026#34;,\u0026#34;scope\u0026#34;:\u0026#34;dapr.runtime.grpc.internal\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-06-19T13:19:53.535259601Z\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;log\u0026#34;,\u0026#34;ver\u0026#34;:\u0026#34;1.10.4\u0026#34;} The errors above were a confirmation that it could not establish a connection because it could not authenticate due to an handshake failure.\nStep 4: Dapr Sentry logs # To dig deeper, I researched how Dapr handles mTLS which pointed me to the Dapr Sentry service.\nThe Dapr Sentry service manages mTLS between services and acts as a certificate authority. It generates mTLS certificates and distributes them to any running sidecars. This allows sidecars to communicate with encrypted, mTLS traffic.\nSo I went to check the Dapr Sentry logs and I finally found the issue: Dapr root certificate expired.\n2023-06-19 14:49:06.566\t{\u0026#34;instance\u0026#34;:\u0026#34;dapr-sentry-123456-abc7\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;warning\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Dapr root certificate expiration warning: certificate has expired.\u0026#34;,\u0026#34;scope\u0026#34;:\u0026#34;dapr.sentry\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-06-19T12:49:06.566339341Z\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;log\u0026#34;,\u0026#34;ver\u0026#34;:\u0026#34;1.10.4\u0026#34;} In order to view the logs of the Dapr Sentry service you can run the following command:\nkubectl logs --selector=app=dapr-sentry --namespace \u0026lt;NAMESPACE\u0026gt; Generating a new root certificate # By default, the certificate expires after 365 days. You can change the expiration time by setting the --cert-chain-expiration flag when you start the Dapr Sentry service. The value is in days.\nWith Dapr, you can encrypt communication between applications using self-signed one valid for 1 year so it was time to renew the certificate.\nTo renew the certificate, I followed the recommended steps to root and issuer certificate upgrade using CLI. You can find the steps here.\nGenerated brand new root and issuer certificates, signed by a newly generated private root key by running the following command: dapr mtls renew-certificate -k --valid-until \u0026lt;days\u0026gt; --restart ⌛ Starting certificate rotation ℹ️ generating fresh certificates ℹ️ Updating certifcates in your Kubernetes cluster ℹ️ Dapr control plane version 1.10.4 detected in namespace dapr-system ✅ Certificate rotation is successful! Your new certicate is valid through Wed, 18 Jun 2025 13:37:30 UTC ℹ️ Restarting deploy/dapr-sentry.. ℹ️ Restarting deploy/dapr-operator.. ℹ️ Restarting statefulsets/dapr-placement-server.. ✅ All control plane services have restarted successfully! Restarted one of the applications in kubernetes to see if the changes worked successfully. And it did!\nRedeployed all applications that were using Dapr via our normal Github Actions pipelines.\nThere was no downtime and the process was quite smooth. Dapr does not renew certificates automatically so depending on your setup you will need to renew them manually or create an intermediary service that does it for you.\nNext steps # Lesson learned no 1: Have an overview of your Dapr services # I had no overview of the Dapr system which caused me a lot of time in trying to get to the root cause. So first thing I did was to create a nice dashboard where we can have an overview of our Dapr services and their certificates. I started from the official one from Grafana for this. But the dashboard is a bit outdated so I had some issues with the queries, so I did some changes and you can find the JSON of the dashboard below if it helps anyone.\nClick here to expand report { \u0026#34;annotations\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;builtIn\u0026#34;: 1, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;enable\u0026#34;: true, \u0026#34;hide\u0026#34;: true, \u0026#34;iconColor\u0026#34;: \u0026#34;rgba(0, 211, 255, 1)\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Annotations \u0026amp; Alerts\u0026#34;, \u0026#34;target\u0026#34;: { \u0026#34;limit\u0026#34;: 100, \u0026#34;matchAny\u0026#34;: false, \u0026#34;tags\u0026#34;: [], \u0026#34;type\u0026#34;: \u0026#34;dashboard\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;dashboard\u0026#34; } ] }, \u0026#34;description\u0026#34;: \u0026#34;This dashboard shows the metrics of Dapr control plane services\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;fiscalYearStartMonth\u0026#34;: 0, \u0026#34;graphTooltip\u0026#34;: 0, \u0026#34;id\u0026#34;: 210, \u0026#34;links\u0026#34;: [], \u0026#34;liveNow\u0026#34;: false, \u0026#34;panels\u0026#34;: [ { \u0026#34;collapsed\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;grafana-azure-monitor-datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;000000006\u0026#34; }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 1, \u0026#34;w\u0026#34;: 24, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 0 }, \u0026#34;id\u0026#34;: 18, \u0026#34;panels\u0026#34;: [], \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;grafana-azure-monitor-datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;000000006\u0026#34; }, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Health \u0026amp; Resource\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;row\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;decimals\u0026#34;: 1, \u0026#34;mappings\u0026#34;: [ { \u0026#34;options\u0026#34;: { \u0026#34;match\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34; } }, \u0026#34;type\u0026#34;: \u0026#34;special\u0026#34; } ], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: 600 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;s\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 8, \u0026#34;w\u0026#34;: 4, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 1 }, \u0026#34;id\u0026#34;: 20, \u0026#34;links\u0026#34;: [], \u0026#34;options\u0026#34;: { \u0026#34;colorMode\u0026#34;: \u0026#34;value\u0026#34;, \u0026#34;graphMode\u0026#34;: \u0026#34;area\u0026#34;, \u0026#34;justifyMode\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;orientation\u0026#34;: \u0026#34;horizontal\u0026#34;, \u0026#34;reduceOptions\u0026#34;: { \u0026#34;calcs\u0026#34;: [ \u0026#34;last\u0026#34; ], \u0026#34;fields\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;values\u0026#34;: false }, \u0026#34;textMode\u0026#34;: \u0026#34;auto\u0026#34; }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.2-cloud.1.94a6f396\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;time() - max(process_start_time_seconds{app=~\\\u0026#34;dapr-sentry|dapr-placement-server|dapr-sidecar-injector|dapr-operator\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}) by (app)\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{app}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Uptime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;stat\u0026#34; }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;This shows total amount of kernel and user CPU usage time.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;links\u0026#34;: [] }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;fill\u0026#34;: 1, \u0026#34;fillGradient\u0026#34;: 0, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 8, \u0026#34;w\u0026#34;: 7, \u0026#34;x\u0026#34;: 4, \u0026#34;y\u0026#34;: 1 }, \u0026#34;hiddenSeries\u0026#34;: false, \u0026#34;id\u0026#34;: 22, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;alertThreshold\u0026#34;: true }, \u0026#34;percentage\u0026#34;: false, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.2-cloud.1.94a6f396\u0026#34;, \u0026#34;pointradius\u0026#34;: 2, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;sum(rate(container_cpu_usage_seconds_total{pod=~\\\u0026#34;(dapr-sentry|dapr-sidecar-injector|dapr-placement-server|dapr-operator).*\\\u0026#34;, cluster=\\\u0026#34;$cluster\\\u0026#34;}[5m])) by (pod)\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{app}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeRegions\u0026#34;: [], \u0026#34;title\u0026#34;: \u0026#34;Total CPU usage (kernel and user)\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;s\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true } ], \u0026#34;yaxis\u0026#34;: { \u0026#34;align\u0026#34;: false } }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;The amount of allocated heap memory that belongs specifically to that process in bytes.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;links\u0026#34;: [] }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;fill\u0026#34;: 1, \u0026#34;fillGradient\u0026#34;: 0, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 8, \u0026#34;w\u0026#34;: 7, \u0026#34;x\u0026#34;: 11, \u0026#34;y\u0026#34;: 1 }, \u0026#34;hiddenSeries\u0026#34;: false, \u0026#34;id\u0026#34;: 24, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;alertThreshold\u0026#34;: true }, \u0026#34;percentage\u0026#34;: false, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.2-cloud.1.94a6f396\u0026#34;, \u0026#34;pointradius\u0026#34;: 2, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;sum(go_memstats_alloc_bytes{app=~\\\u0026#34;(dapr-sentry|dapr-sidecar-injector|dapr-placement-server|dapr-operator)\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) by (app)\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{app}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeRegions\u0026#34;: [], \u0026#34;title\u0026#34;: \u0026#34;Heap Memory usage in bytes\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true } ], \u0026#34;yaxis\u0026#34;: { \u0026#34;align\u0026#34;: false } }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;links\u0026#34;: [] }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;fill\u0026#34;: 1, \u0026#34;fillGradient\u0026#34;: 0, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 8, \u0026#34;w\u0026#34;: 6, \u0026#34;x\u0026#34;: 18, \u0026#34;y\u0026#34;: 1 }, \u0026#34;hiddenSeries\u0026#34;: false, \u0026#34;id\u0026#34;: 26, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;alertThreshold\u0026#34;: true }, \u0026#34;percentage\u0026#34;: false, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.2-cloud.1.94a6f396\u0026#34;, \u0026#34;pointradius\u0026#34;: 2, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;max(go_goroutines{app=~\\\u0026#34;(dapr-sentry|dapr-sidecar-injector|dapr-placement-server|dapr-operator)\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) by (app)\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{app}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeRegions\u0026#34;: [], \u0026#34;title\u0026#34;: \u0026#34;Number of GO routines\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true }, { \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true } ], \u0026#34;yaxis\u0026#34;: { \u0026#34;align\u0026#34;: false } }, { \u0026#34;collapsed\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;grafana-azure-monitor-datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;000000006\u0026#34; }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 1, \u0026#34;w\u0026#34;: 24, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 9 }, \u0026#34;id\u0026#34;: 12, \u0026#34;panels\u0026#34;: [], \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;grafana-azure-monitor-datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;000000006\u0026#34; }, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Sidecar Injector\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;row\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;The total number of sidecar injection requests.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 10, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 1, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;never\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;links\u0026#34;: [], \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;short\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 9, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 10 }, \u0026#34;id\u0026#34;: 8, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [], \u0026#34;displayMode\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;bottom\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.1-cloud.3.f250259e\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;exemplar\u0026#34;: false, \u0026#34;expr\u0026#34;: \u0026#34;rate(dapr_injector_sidecar_injection_requests_total{cluster=\\\u0026#34;$cluster\\\u0026#34;}[5m])\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{app}},{{pod}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;# sidecar injection requests\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;The total number of successful sidecar injection requests.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 10, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 1, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;never\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;links\u0026#34;: [], \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;short\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 9, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 12, \u0026#34;y\u0026#34;: 10 }, \u0026#34;id\u0026#34;: 10, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [], \u0026#34;displayMode\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;bottom\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.1-cloud.3.f250259e\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;rate(dapr_injector_sidecar_injection_succeeded_total{cluster=\\\u0026#34;$cluster\\\u0026#34;}[5m])\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{app_id}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;# successful sidecar injected\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;collapsed\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;grafana-azure-monitor-datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;000000006\u0026#34; }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 1, \u0026#34;w\u0026#34;: 24, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 19 }, \u0026#34;id\u0026#34;: 42, \u0026#34;panels\u0026#34;: [], \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;grafana-azure-monitor-datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;000000006\u0026#34; }, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;CA Sentry\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;row\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;thresholds\u0026#34; }, \u0026#34;mappings\u0026#34;: [ { \u0026#34;options\u0026#34;: { \u0026#34;match\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;N/A\u0026#34; } }, \u0026#34;type\u0026#34;: \u0026#34;special\u0026#34; } ], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;#F2495C\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;#FADE2A\u0026#34;, \u0026#34;value\u0026#34;: 2628000 }, { \u0026#34;color\u0026#34;: \u0026#34;#73BF69\u0026#34;, \u0026#34;value\u0026#34;: 5256000 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;s\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 7, \u0026#34;w\u0026#34;: 3, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 20 }, \u0026#34;id\u0026#34;: 44, \u0026#34;links\u0026#34;: [], \u0026#34;maxDataPoints\u0026#34;: 100, \u0026#34;options\u0026#34;: { \u0026#34;colorMode\u0026#34;: \u0026#34;value\u0026#34;, \u0026#34;graphMode\u0026#34;: \u0026#34;area\u0026#34;, \u0026#34;justifyMode\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;orientation\u0026#34;: \u0026#34;horizontal\u0026#34;, \u0026#34;reduceOptions\u0026#34;: { \u0026#34;calcs\u0026#34;: [ \u0026#34;lastNotNull\u0026#34; ], \u0026#34;fields\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;values\u0026#34;: false }, \u0026#34;textMode\u0026#34;: \u0026#34;auto\u0026#34; }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.2-cloud.1.94a6f396\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;min(dapr_sentry_issuercert_expiry_timestamp) - time()\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;__auto\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;timeFrom\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Root/Issuer cert expiry\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;stat\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Certificate Signing Request ( CSR ) from Dapr runtime\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 0, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 2, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;never\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;links\u0026#34;: [], \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;short\u0026#34; }, \u0026#34;overrides\u0026#34;: [ { \u0026#34;matcher\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;byName\u0026#34;, \u0026#34;options\u0026#34;: \u0026#34;CSR Requests\u0026#34; }, \u0026#34;properties\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;color\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;fixedColor\u0026#34;: \u0026#34;rgb(60, 33, 166)\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;fixed\u0026#34; } }, { \u0026#34;id\u0026#34;: \u0026#34;custom.lineStyle\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;dash\u0026#34;: [ 10, 10 ], \u0026#34;fill\u0026#34;: \u0026#34;dash\u0026#34; } } ] }, { \u0026#34;matcher\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;byName\u0026#34;, \u0026#34;options\u0026#34;: \u0026#34;CSR Success\u0026#34; }, \u0026#34;properties\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;color\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;fixedColor\u0026#34;: \u0026#34;#73BF69\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;fixed\u0026#34; } } ] }, { \u0026#34;matcher\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;byName\u0026#34;, \u0026#34;options\u0026#34;: \u0026#34;CSR Failure\u0026#34; }, \u0026#34;properties\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;color\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;fixedColor\u0026#34;: \u0026#34;#F2495C\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;fixed\u0026#34; } } ] } ] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 7, \u0026#34;w\u0026#34;: 9, \u0026#34;x\u0026#34;: 3, \u0026#34;y\u0026#34;: 20 }, \u0026#34;id\u0026#34;: 34, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [], \u0026#34;displayMode\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;right\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.1-cloud.3.f250259e\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;exemplar\u0026#34;: false, \u0026#34;expr\u0026#34;: \u0026#34;sum(dapr_sentry_cert_sign_request_received_total{app=\\\u0026#34;dapr-sentry\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;CSR Requests\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;sum(dapr_sentry_cert_sign_success_total{app=\\\u0026#34;dapr-sentry\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;CSR Success\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;sum(dapr_sentry_cert_sign_failure_total{app=\\\u0026#34;dapr-sentry\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;})\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;CSR Failure\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Certificate Signing Requests (CSR) from Daprd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;This chart shows the failure reason of Certificate Sign Request.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 10, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 1, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;never\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;links\u0026#34;: [], \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;short\u0026#34; }, \u0026#34;overrides\u0026#34;: [ { \u0026#34;__systemRef\u0026#34;: \u0026#34;hideSeriesFrom\u0026#34;, \u0026#34;matcher\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;byNames\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;exclude\u0026#34;, \u0026#34;names\u0026#34;: [ \u0026#34;req_id_validation\u0026#34; ], \u0026#34;prefix\u0026#34;: \u0026#34;All except:\u0026#34;, \u0026#34;readOnly\u0026#34;: true } }, \u0026#34;properties\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;custom.hideFrom\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: true } } ] } ] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 7, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 12, \u0026#34;y\u0026#34;: 20 }, \u0026#34;id\u0026#34;: 38, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [], \u0026#34;displayMode\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;right\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.1-cloud.3.f250259e\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;sum(dapr_sentry_cert_sign_failure_total{app=\\\u0026#34;dapr-sentry\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) by (reason)\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{reason}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;CSR Failures\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;This will be counted when issuer cert and key are changed.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;thresholds\u0026#34; }, \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] } }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 7, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 27 }, \u0026#34;id\u0026#34;: 36, \u0026#34;links\u0026#34;: [], \u0026#34;options\u0026#34;: { \u0026#34;colorMode\u0026#34;: \u0026#34;value\u0026#34;, \u0026#34;graphMode\u0026#34;: \u0026#34;area\u0026#34;, \u0026#34;justifyMode\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;orientation\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;reduceOptions\u0026#34;: { \u0026#34;calcs\u0026#34;: [ \u0026#34;lastNotNull\u0026#34; ], \u0026#34;fields\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;values\u0026#34;: false }, \u0026#34;textMode\u0026#34;: \u0026#34;auto\u0026#34; }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.2-cloud.1.94a6f396\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;sum(dapr_sentry_issuercert_changed_total{app=\\\u0026#34;dapr-sentry\\\u0026#34;})\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Issuer cert and key changed total\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;stat\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;This chart shows the reason of gRPC server TLS certificate issuance failures.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 10, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 1, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;never\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;links\u0026#34;: [], \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;short\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 7, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 12, \u0026#34;y\u0026#34;: 27 }, \u0026#34;id\u0026#34;: 40, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [], \u0026#34;displayMode\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;right\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.1-cloud.3.f250259e\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;sum(dapr_sentry_servercert_issue_failed_total{app=\\\u0026#34;dapr-sentry\\\u0026#34;,cluster=\\\u0026#34;$cluster\\\u0026#34;}) by (reason)\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{reason}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Server TLS certificate issuance failures\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;collapsed\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;grafana-azure-monitor-datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;000000006\u0026#34; }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 1, \u0026#34;w\u0026#34;: 24, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 34 }, \u0026#34;id\u0026#34;: 16, \u0026#34;panels\u0026#34;: [], \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;grafana-azure-monitor-datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;000000006\u0026#34; }, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Placement\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;row\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;The total number of Dapr sidecars connected to placement service.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 10, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 1, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;never\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;links\u0026#34;: [], \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;short\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 8, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 35 }, \u0026#34;id\u0026#34;: 28, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [], \u0026#34;displayMode\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;bottom\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.1-cloud.3.f250259e\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;dapr_placement_runtimes_total{cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{sidecar}}\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;Number of connected Dapr sidecars\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;The total number of actor sidecars and non-actor sidecars.\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 10, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 1, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;never\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;links\u0026#34;: [], \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;short\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 8, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 12, \u0026#34;y\u0026#34;: 35 }, \u0026#34;id\u0026#34;: 30, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [], \u0026#34;displayMode\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;bottom\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.0.1-cloud.3.f250259e\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;dapr_placement_actor_runtimes_total{cluster=\\\u0026#34;$cluster\\\u0026#34;}\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;actor sidecars\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;dapr_placement_runtimes_total-dapr_placement_actor_runtimes_total\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;non actor sidecars\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;actor sidecars vs non-actor sidecars\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; } ], \u0026#34;refresh\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;schemaVersion\u0026#34;: 38, \u0026#34;style\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;templating\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;current\u0026#34;: { \u0026#34;selected\u0026#34;: true, \u0026#34;text\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${monitoring_datasource_name}\u0026#34; }, \u0026#34;definition\u0026#34;: \u0026#34;label_values(cluster)\u0026#34;, \u0026#34;hide\u0026#34;: 0, \u0026#34;includeAll\u0026#34;: false, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;multi\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;cluster\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;query\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;label_values(cluster)\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;PrometheusVariableQueryEditor-VariableQuery\u0026#34; }, \u0026#34;refresh\u0026#34;: 1, \u0026#34;regex\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;skipUrlSync\u0026#34;: false, \u0026#34;sort\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;query\u0026#34; } ] }, \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-2d\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;5s\u0026#34;, \u0026#34;10s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;30m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;2h\u0026#34;, \u0026#34;1d\u0026#34; ] }, \u0026#34;timezone\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Dapr System Services Dashboard\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;RHSwiHXWk\u0026#34;, \u0026#34;version\u0026#34;: 16, \u0026#34;weekStart\u0026#34;: \u0026#34;\u0026#34; } I added some variables for the Prometheus datasource name and the cluster name. You can change the refresh rate of the dashboard and the time range.\nAnd the output creates a dashboard that something like below:\nEach section of the dashboard has a very nice info box that will tell you what the section shows and how to interpret the data.\nIdeally I think a good practice is not to overload yourself by creating tons of dashboards that you will not look at, maintain or forget about.\nIn this case, it\u0026rsquo;s quite useful to have one because in the event of an incident or a problem, this will save you hours of troubleshooting and will give you a good overview of the system and what is failing. If you look at the outputs of the board itself you\u0026rsquo;ll see that it logs: CSR Failures, Server TLS certificate issuance failures, etc.\nLesson Learned no 2: Make sure you are aware before expiration # Beginning 30 days prior to mTLS root certificate expiration the Dapr sentry service will emit hourly warning level logs indicating that the root certificate is about to expire. You can use these logs to set up alerts to notify you when the certificate is about to expire.\n\u0026#34;Dapr root certificate expiration warning: certificate expires in 2 days and 15 hours\u0026#34; First thing is configure a Loki data source.\nI already had this done and setting it up might be the subject of another blog post. But in a nutshell, Loki is a log aggregation system that integrates with Grafana which allows you to ingest and query log data. So I just made sure I had a Loki data source configured correctly.\nNext, I created a create a log query.\nIn the Explore view of Grafana, selecting the Loki data source, I wrote a log query that retrieves the logs I want to use for the alert. The query you build might differ but it should match the logs produced by the kubectl logs command for dapr-sentry.\nFor example:\n{cluster=\u0026#34;$cluster\u0026#34;, namespace=\u0026#34;dapr-system\u0026#34;} |= `Dapr root certificate expiration warning: certificate expires in` Adjust the query based on the specific log lines or patterns you want to target. I wanted to get all the logs that had the warning message about the certificate expiration starting from the 30days mark. But you can just edit the query to log you x days before the expiration.\nA good rule of thumb is to test the log query.\nAfter executing the query, you should see the warnings in the log entries.\nTip:\nIf no logs are returned, check that the query is correct, that the data source is set up correctly, and that the logs are being ingested by Loki.\nSince all was good in my case, I proceeded to add this query from the Explore page to my previously created dashboard so I can see the logs in the dashboard itself as well. So I created a new panel with the logs and a nice description of what the logs mean for anyone reading it.\nAnd lastly, I create an alert rule.\nIn the Alerting section in Grafana I went to \u0026ldquo;Create Rule\u0026rdquo; to define an alert rule. I configured the alert based on the previous query and I defined the conditions that trigger the alert based on the log query results. For example, you can set a condition like \u0026ldquo;Count() is above 0\u0026rdquo; to trigger the alert when there is at least one log entry matching the query. Or you can customize it based on your needs.\nHere the implementation of the alert might differ based on what tooling you use, which channel you want to be alerted on (slack, email etc).\nHope this gave an insight into how you can troubleshoot and monitor Dapr in your environments.\nThank you for reading! And let me know if you have any questions or feedback.\n","date":"18 July 2023","externalUrl":null,"permalink":"/posts/dapr-certificate-renewal/","section":"Posts","summary":"A CNCF project, the Distributed Application Runtime (Dapr) provides APIs that simplify microservice connectivity.","title":"Ensuring Seamless Operations: Troubleshooting and Resolving Dapr Certificate Expiry","type":"posts"},{"content":"","date":"18 July 2023","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"Currently, there is no option for adding a Group as an Owner to a Service Principal or Azure AD Application.\nIf you try you will most likely run into the following error:\nunexpected status 400 with OData error: Request_BadRequest: The reference target │ \u0026#39;Group_0000000-0000-000000-000000\u0026#39; of type \u0026#39;Group\u0026#39; is invalid for the \u0026#39;owners\u0026#39; reference. This is a quick post on how you can add members of an Az AD Group as Owners of a Service Principal or Azure AD Application. This is useful if you want to give your team members access to the Service Principal or Azure AD Application without giving them access to the Azure Subscription.\nIt\u0026rsquo;s also useful if you want to manage ownership of the Service Principal or Azure AD Application dynamically. For example, if a team member leaves or joins the company you can just add or remove them from the group and this will automatically update the ownership of the Service Principal or Azure AD Application.\nThe Terraform code below assumes you already have the group created in Azure AD.\n## Adding the current subscription and Azure AD client data sources data \u0026#34;azurerm_subscription\u0026#34; \u0026#34;current\u0026#34; {} data \u0026#34;azuread_client_config\u0026#34; \u0026#34;current\u0026#34; {} ## Adding the group data source of the members we want to retrieve data \u0026#34;azuread_group\u0026#34; \u0026#34;group_name\u0026#34; { display_name = \u0026#34;Organization.GroupName\u0026#34; } ## Retrieve the list of group member ids locals { group_member_object_ids = toset(concat([data.azuread_client_config.current.object_id],data.azuread_group.group_name.members)) } ## Adding the group member object ids to the owners of the application resource \u0026#34;azuread_application\u0026#34; \u0026#34;application\u0026#34; { display_name = \u0026#34;${var.name}-app\u0026#34; owners = local.group_member_object_ids app_role { allowed_member_types = [\u0026#34;User\u0026#34;, \u0026#34;Application\u0026#34;] (...) } } ## Adding the group member object ids to the owners of the service principal resource \u0026#34;azuread_service_principal\u0026#34; \u0026#34;service_principal\u0026#34; { application_id = azuread_application.application.application_id owners = local.group_member_object_ids } Hope this helps someone out there!\n","date":"21 April 2023","externalUrl":null,"permalink":"/posts/terraform-add-group-members-as-owners/","section":"Posts","summary":"Currently, there is no option for adding a Group as an Owner to a Service Principal or Azure AD Application.","title":"Terraform: Add Group Members as Owners to a Service Principal or Az AD Application","type":"posts"},{"content":"If you work with real data and customers you will have to deal with customer support tickets. This is a very important part of your business and you should take it seriously. In this article, I will give my lessons learned and some tips on how to handle customer support tickets.\nWhat is a customer support ticket? # A customer support ticket is a question, a bug report, a feature request or a complaint coming from a customer of your product. It is a communication channel between you and your customer. These complaints are logged in a ticketing tracking system such as JIRA. Generally, the expectation is to handle these tickets promptly and to make sure that your customer is happy.\nMore often than not, they are given priority and you have to handle them in order of priority. You might even have SLAs tied to them and that means based on the priority you have to handle them within a certain time frame.\nWhy is it important? # This is from the customer\u0026rsquo;s point of view, but customer support tickets can also be a distraction to your software development team, adding cognitive load to your team and slowing down the development process. If not handled correctly, they can also be a source of frustration for your team. And even lead to burnout.\nThis can be avoided by having a good process in place and by having a good understanding of the customer\u0026rsquo;s needs.\nThe following are some of the Do\u0026rsquo;s and Don\u0026rsquo;ts that I have learned over the years while navigating the trenches of customer support tickets and having experienced a lot of pain myself.\nDo\u0026rsquo;s # 1. Do have a correct prioritization of tickets # The first thing you need to do is to have a correct prioritization of tickets. This is very important because it will help you to focus on the most important tickets and to avoid distractions. When the workload is high, asking a team to focus on the lowest-priority tickets is a recipe for disaster.\nBut understanding what makes a ticket a P1 (Priority 1 - should be taken immediately) or a P4 (Can wait 2-3 days) needs to be understood by all teams working with the ticketing system and the ticket itself. This means different things for the stakeholders involved in the ticket:\ncustomer support : they are assessing the impact of the issue on the customer, they need to understand clearly how the customer is impacted and what is the urgency of the ticket software development : they need to understand the impact of the issue on the product and what time frame they have to fix it product management : they need to make sure the SLAs are met and that the customer is happy Tips on how you can achieve this:\nHave a clear internal definition of what a P1, P2, P3, P4 ticket is Have the definitions clearly defined in a document and make sure that everyone is aware of it and is easily accessible (make sure you are not relying on tribal knowledge). Provide training often to make sure that everyone is on the same page 2. Do have a clear escalation process in place # This is two-fold. From the organization\u0026rsquo;s point of view have a clear process in place on how to escalate tickets, who is responsible and who is the next person in contact. Make sure these roles are clearly defined and visible to everyone in the company.\nIf customer support people are tagging teams in the ticket, make sure that the team\u0026rsquo;s competency is clear and the boundaries are set and all parties responsible are aware of it. If unsure, don\u0026rsquo;t hesitate to ask the team directly. Be careful: you are asking about the team\u0026rsquo;s domain, not help on the issue itself!\nOtherwise, you will end up with a lot of back-and-forth between teams, losing time that could\u0026rsquo;ve been put into investigating the issue and fixing it. Moreover, you might end up wasting unnecessary time with the wrong team causing frustration and eventually a lack of trust and cooperation between the teams. Everyone has tasks to do and they need to be able to focus on them. So being mindful of their time is basic respect.\nFrom the software team\u0026rsquo;s point of view create an escalation process in the team as well. Tips on how to achieve this:\nDivide the tickets among the team Create a rotation and make sure that everyone is aware of it Make sure there is an escalation process inside the team as well: juniors take easy tickets, quick wins, etc. and seniors take the more complex ones, when in doubt they can ask for help from the seniors. Same for the seniors, ask for help from the lead or the point of contact if you are unsure of how to proceed, if there is a delay in fixing the issue, if there needs to be communication with the customer, etc. 3. Do create an internal knowledge base # I cannot stress how important this is. You need to have a place where you can store all the knowledge you have gathered over the years. This is a very important source of information and it will help you to avoid repeating the same mistakes over and over again. It will also help you to avoid having to rely on tribal knowledge.\nTip It doesn\u0026rsquo;t matter where you have it: Confluence pages, a Google Doc, a wiki, a GitHub repo, a Notion page, workbooks etc. The important thing is that you have it written down, it\u0026rsquo;s up-to-date and you are using it.\nThis will also help with the onboarding process. You can slowly introduce new members of the team or junior members of the team to the knowledge base and they will be able to learn from it and handle low-level tickets on their own. This also means you are freeing up your more senior members of the team to focus on more complex issues. Win-win!\nTip In order to achieve this, you need team buy-in and commitment to growth. As a team lead or manager, you need to make sure that everyone is aware of the importance of this and that they are willing to contribute to it for the betterment of the team. You need to make sure that everyone is aware of the knowledge base and that they are using it. You need to make sure that everyone is aware of the importance of updating it.\nIf there is one thing you do, do this with your software development team. It will save you a lot of time and frustration.\n4. Do check for patterns in the tickets # Have you seen an increase in the number of tickets related to a specific feature, browser, OS, location, etc? If the answer is yes, then you need to evaluate and investigate why this is happening. It could be a bug, a regression, a change in the product, a change in the customer\u0026rsquo;s environment, a change in the customer\u0026rsquo;s behavior, etc.\nIt can even be an issue that has not manifested itself as a failure yet, but there is a potential for it. As an SRE, you need to be aware of this and be proactive in preventing it. This is a good opportunity to improve the product and to make it more resilient.\nIt can also mean the users are changing the way they use the product: either they find the functionality hard to use, don\u0026rsquo;t understand it or they are using it in a way that was not intended. This is a good opportunity to improve the product and to make it more user-friendly.\nTip Review them as a team and make a resolution: dismiss it as a one-off, investigate it, fix it, improve the product, etc. And set ownership: there is no point in giving a user behavior change pattern to an engineer, let the product owner or the product manager investigate it. Keep it divided and efficient. You get my point.\nDon\u0026rsquo;ts # 1. Don\u0026rsquo;t rely on tribal knowledge # Tribal knowledge is a very dangerous and fragile thing. It is also a very toxic thing because well check the next point on this one. It is a very bad thing. How can I reiterate this enough?\nYou will lose so much time trying to figure out who worked on a similar issue, what was the cause, and how did they fix it. And you will never learn from it because once the person who worked on it leaves, you will lose all that knowledge. So instead of learning from it, you will have to fix the same issue over and over again. That\u0026rsquo;s just running in circles.\n2. Don\u0026rsquo;t have a single point of failure/ single person responsible for handling tickets # If you use tribal knowledge, you will end up with a single point of failure.\nWe LOVE heroes. But unless it\u0026rsquo;s Scarlet Witch, Black Widow or Ironman (yes, I know), heroes have no place in a team in my opinion.\nWhat a \u0026ldquo;hero\u0026rdquo; will do is fix issues without disclosing the root cause or sharing the knowledge with the rest of the team. This is a very bad practice. What if the person leaves the company? What if the person is on vacation? What if the person is sick? What if the person is overloaded with tickets and cannot handle them all? What if the person is not available for some reason?\nAssuming the person is not a \u0026lsquo;hero\u0026rsquo; and is just a hard worker, it will also lead to a lot of frustration and eventually burnout for the person that will be contacted all the time. We cannot expect people to give 100% when we flood them with tickets and we don\u0026rsquo;t give them the tools to succeed.\nTip When we see an engineer brilliant at debugging complex systems, the tendency is to give them more tickets to handle because they can \u0026lsquo;fix it fast\u0026rsquo;. Don\u0026rsquo;t. That person is a precious resource that should be given only the most complex tickets. Leave the groundwork to the rest of the team. This will also help with the onboarding process and will help the team to grow.\n3. Don\u0026rsquo;t let the customer support people assume the issue in the ticket is the root cause # Alrighty! Customer support people are not technical people. This is not a bad thing, their biggest asset is communicating with the clients. They are the ones that are in direct contact with the clients and they are the ones that know the clients the best. They are the ones that can provide the most valuable information about the issue.\nThe tempting thing for them to do is to not describe the issue in the ticket, what the user is seeing, but, based on similarity to other tickets fixed, assume it has the same root cause and write that one down as the issue the user is facing.\nThis can actually cause more confusion for the engineering team because you are sending them on a different path to the issue.\nKeep it simple: describe the issue with as many details as you can, add steps to reproduce, add screenshots, add logs, and add anything that can help the engineering team to understand the issue and to fix it. And stop, don\u0026rsquo;t assume anything. Trust the engineering team to figure it out.\nTip: A good practice to prevent this is to have a template for the tickets. This will help the customer support people provide all the necessary information and it will help the engineering team to understand the issue.\n4. Don\u0026rsquo;t ignore patterns in the customer support tickets # They will come and haunt you. You will have to deal with them eventually.\nIt can come back as anything from a system failure to losing business on the platform because the users find it too hard to use. It can also come back as a loss of reputation because the users are not happy with the product.\nConclusion # Have simple, clear processes and procedures in place: organization level and team level. Share the responsibilities and the ownership of the tickets. Have a knowledge base. Check for patterns in the tickets. Train your teams - often until they are comfortable.\nHope this helps! Let me know what you think in the comments below.\n","date":"9 February 2023","externalUrl":null,"permalink":"/coffee/handling-customer-support-tickets/","section":"Coffee","summary":"If you work with real data and customers you will have to deal with customer support tickets.","title":"Navigating the Trenches: A Guide to Effortlessly Handling Customer Support Tickets","type":"coffee"},{"content":"If you work with real data and customers you will have to deal with customer support tickets. This is a very important part of your business and you should take it seriously. In this article, I will give my lessons learned and some tips on how to handle customer support tickets.\nWhat is a customer support ticket? # A customer support ticket is a question, a bug report, a feature request or a complaint coming from a customer of your product. It is a communication channel between you and your customer. These complaints are logged in a ticketing tracking system such as JIRA. Generally, the expectation is to handle these tickets promptly and to make sure that your customer is happy.\nMore often than not, they are given priority and you have to handle them in order of priority. You might even have SLAs tied to them and that means based on the priority you have to handle them within a certain time frame.\nWhy is it important? # This is from the customer\u0026rsquo;s point of view, but customer support tickets can also be a distraction to your software development team, adding cognitive load to your team and slowing down the development process. If not handled correctly, they can also be a source of frustration for your team. And even lead to burnout.\nThis can be avoided by having a good process in place and by having a good understanding of the customer\u0026rsquo;s needs.\nThe following are some of the Do\u0026rsquo;s and Don\u0026rsquo;ts that I have learned over the years while navigating the trenches of customer support tickets and having experienced a lot of pain myself.\nDo\u0026rsquo;s # 1. Do have a correct prioritization of tickets # The first thing you need to do is to have a correct prioritization of tickets. This is very important because it will help you to focus on the most important tickets and to avoid distractions. When the workload is high, asking a team to focus on the lowest-priority tickets is a recipe for disaster.\nBut understanding what makes a ticket a P1 (Priority 1 - should be taken immediately) or a P4 (Can wait 2-3 days) needs to be understood by all teams working with the ticketing system and the ticket itself. This means different things for the stakeholders involved in the ticket:\ncustomer support : they are assessing the impact of the issue on the customer, they need to understand clearly how the customer is impacted and what is the urgency of the ticket software development : they need to understand the impact of the issue on the product and what time frame they have to fix it product management : they need to make sure the SLAs are met and that the customer is happy Tips on how you can achieve this:\nHave a clear internal definition of what a P1, P2, P3, P4 ticket is Have the definitions clearly defined in a document and make sure that everyone is aware of it and is easily accessible (make sure you are not relying on tribal knowledge). Provide training often to make sure that everyone is on the same page 2. Do have a clear escalation process in place # This is two-fold. From the organization\u0026rsquo;s point of view have a clear process in place on how to escalate tickets, who is responsible and who is the next person in contact. Make sure these roles are clearly defined and visible to everyone in the company.\nIf customer support people are tagging teams in the ticket, make sure that the team\u0026rsquo;s competency is clear and the boundaries are set and all parties responsible are aware of it. If unsure, don\u0026rsquo;t hesitate to ask the team directly. Be careful: you are asking about the team\u0026rsquo;s domain, not help on the issue itself!\nOtherwise, you will end up with a lot of back-and-forth between teams, losing time that could\u0026rsquo;ve been put into investigating the issue and fixing it. Moreover, you might end up wasting unnecessary time with the wrong team causing frustration and eventually a lack of trust and cooperation between the teams. Everyone has tasks to do and they need to be able to focus on them. So being mindful of their time is basic respect.\nFrom the software team\u0026rsquo;s point of view create an escalation process in the team as well. Tips on how to achieve this:\nDivide the tickets among the team Create a rotation and make sure that everyone is aware of it Make sure there is an escalation process inside the team as well: juniors take easy tickets, quick wins, etc. and seniors take the more complex ones, when in doubt they can ask for help from the seniors. Same for the seniors, ask for help from the lead or the point of contact if you are unsure of how to proceed, if there is a delay in fixing the issue, if there needs to be communication with the customer, etc. 3. Do create an internal knowledge base # I cannot stress how important this is. You need to have a place where you can store all the knowledge you have gathered over the years. This is a very important source of information and it will help you to avoid repeating the same mistakes over and over again. It will also help you to avoid having to rely on tribal knowledge.\nTip It doesn\u0026rsquo;t matter where you have it: Confluence pages, a Google Doc, a wiki, a GitHub repo, a Notion page, workbooks etc. The important thing is that you have it written down, it\u0026rsquo;s up-to-date and you are using it.\nThis will also help with the onboarding process. You can slowly introduce new members of the team or junior members of the team to the knowledge base and they will be able to learn from it and handle low-level tickets on their own. This also means you are freeing up your more senior members of the team to focus on more complex issues. Win-win!\nTip In order to achieve this, you need team buy-in and commitment to growth. As a team lead or manager, you need to make sure that everyone is aware of the importance of this and that they are willing to contribute to it for the betterment of the team. You need to make sure that everyone is aware of the knowledge base and that they are using it. You need to make sure that everyone is aware of the importance of updating it.\nIf there is one thing you do, do this with your software development team. It will save you a lot of time and frustration.\n4. Do check for patterns in the tickets # Have you seen an increase in the number of tickets related to a specific feature, browser, OS, location, etc? If the answer is yes, then you need to evaluate and investigate why this is happening. It could be a bug, a regression, a change in the product, a change in the customer\u0026rsquo;s environment, a change in the customer\u0026rsquo;s behavior, etc.\nIt can even be an issue that has not manifested itself as a failure yet, but there is a potential for it. As an SRE, you need to be aware of this and be proactive in preventing it. This is a good opportunity to improve the product and to make it more resilient.\nIt can also mean the users are changing the way they use the product: either they find the functionality hard to use, don\u0026rsquo;t understand it or they are using it in a way that was not intended. This is a good opportunity to improve the product and to make it more user-friendly.\nTip Review them as a team and make a resolution: dismiss it as a one-off, investigate it, fix it, improve the product, etc. And set ownership: there is no point in giving a user behavior change pattern to an engineer, let the product owner or the product manager investigate it. Keep it divided and efficient. You get my point.\nDon\u0026rsquo;ts # 1. Don\u0026rsquo;t rely on tribal knowledge # Tribal knowledge is a very dangerous and fragile thing. It is also a very toxic thing because well check the next point on this one. It is a very bad thing. How can I reiterate this enough?\nYou will lose so much time trying to figure out who worked on a similar issue, what was the cause, and how did they fix it. And you will never learn from it because once the person who worked on it leaves, you will lose all that knowledge. So instead of learning from it, you will have to fix the same issue over and over again. That\u0026rsquo;s just running in circles.\n2. Don\u0026rsquo;t have a single point of failure/ single person responsible for handling tickets # If you use tribal knowledge, you will end up with a single point of failure.\nWe LOVE heroes. But unless it\u0026rsquo;s Scarlet Witch, Black Widow or Ironman (yes, I know), heroes have no place in a team in my opinion.\nWhat a \u0026ldquo;hero\u0026rdquo; will do is fix issues without disclosing the root cause or sharing the knowledge with the rest of the team. This is a very bad practice. What if the person leaves the company? What if the person is on vacation? What if the person is sick? What if the person is overloaded with tickets and cannot handle them all? What if the person is not available for some reason?\nAssuming the person is not a \u0026lsquo;hero\u0026rsquo; and is just a hard worker, it will also lead to a lot of frustration and eventually burnout for the person that will be contacted all the time. We cannot expect people to give 100% when we flood them with tickets and we don\u0026rsquo;t give them the tools to succeed.\nTip When we see an engineer brilliant at debugging complex systems, the tendency is to give them more tickets to handle because they can \u0026lsquo;fix it fast\u0026rsquo;. Don\u0026rsquo;t. That person is a precious resource that should be given only the most complex tickets. Leave the groundwork to the rest of the team. This will also help with the onboarding process and will help the team to grow.\n3. Don\u0026rsquo;t let the customer support people assume the issue in the ticket is the root cause # Alrighty! Customer support people are not technical people. This is not a bad thing, their biggest asset is communicating with the clients. They are the ones that are in direct contact with the clients and they are the ones that know the clients the best. They are the ones that can provide the most valuable information about the issue.\nThe tempting thing for them to do is to not describe the issue in the ticket, what the user is seeing, but, based on similarity to other tickets fixed, assume it has the same root cause and write that one down as the issue the user is facing.\nThis can actually cause more confusion for the engineering team because you are sending them on a different path to the issue.\nKeep it simple: describe the issue with as many details as you can, add steps to reproduce, add screenshots, add logs, and add anything that can help the engineering team to understand the issue and to fix it. And stop, don\u0026rsquo;t assume anything. Trust the engineering team to figure it out.\nTip: A good practice to prevent this is to have a template for the tickets. This will help the customer support people provide all the necessary information and it will help the engineering team to understand the issue.\n4. Don\u0026rsquo;t ignore patterns in the customer support tickets # They will come and haunt you. You will have to deal with them eventually.\nIt can come back as anything from a system failure to losing business on the platform because the users find it too hard to use. It can also come back as a loss of reputation because the users are not happy with the product.\nConclusion # Have simple, clear processes and procedures in place: organization level and team level. Share the responsibilities and the ownership of the tickets. Have a knowledge base. Check for patterns in the tickets. Train your teams - often until they are comfortable.\nHope this helps! Let me know what you think in the comments below.\n","date":"9 February 2023","externalUrl":null,"permalink":"/reads/handling-customer-support-tickets/","section":"Reads","summary":"If you work with real data and customers you will have to deal with customer support tickets.","title":"Navigating the Trenches: A Guide to Effortlessly Handling Customer Support Tickets","type":"reads"},{"content":"","date":"9 February 2023","externalUrl":null,"permalink":"/tags/on-call/","section":"Tags","summary":"","title":"On-Call","type":"tags"},{"content":"","date":"9 February 2023","externalUrl":null,"permalink":"/tags/sre/","section":"Tags","summary":"","title":"Sre","type":"tags"},{"content":"The other day I was making changes to my helm charts and, after deploying my application, I noticed that one of my pods was stuck in a CreateContainerConfigError state. This is a pretty tricky error because it doesn\u0026rsquo;t give you any details on what the underlying issue could be.\nWhat is the CreateContainerConfigError? # To understand this, let\u0026rsquo;s look at what happens at deployment time to give you an idea of the flow and what could go wrong at each step.\nWhen you deploy a pod, the first step is to pull the image from the registry and then create the container. If the image is not found, then Kubernetes will return an ErrImagePull error. If the image is found, then it will proceed to create the container.\nIf the container creation fails, then it will return a CreateContainerError error. If the container creation succeeds, then Kubernetes will then start the container.\nIf the container start fails, then it will return a CreateContainerConfigError error.\nIn other words, the error happens when the container is transitioning from a Pending state to a Running state. It is at this point that the deployment configuration will be validated to make sure that the container can be started. If the configuration is invalid, then it will return a CreateContainerConfigError error.\nHow to Troubleshoot the CreateContainerConfigError # Disclaimer: There can be many reasons why the container configuration is invalid and it will depende on your specific configuration. I will only be covering the one that I have encountered. If you have encountered a different cause, please leave a comment below.\nBecause the error happens during the validation of the configuration, a good starting point is to double-check the following:\nis the ConfigMap missing? Is it properly configured? is a Secret missing? Is it properly configured? is the PersistentVolume missing? Is it properly configured? is the Pod being created correctly? Are there any empty or invalid fields? Now that we understand what the error is, and what we should be looking at, let\u0026rsquo;s look at how to troubleshoot it and narrow down the problem.\nCheck the Pod Status # The first thing I did was get the pod that I had the error and that I wanted to drill into.\nYou can do this by running kubectl get pods -n \u0026lt;namespace\u0026gt;.\n~ kubectl get pods -n my-service NAME READY STATUS RESTARTS AGE my-service-00000000078c9fff-dssbk 0/2 CreateContainerConfigError 1 (10s ago) 28s my-service-00000000bcddf7d-xfsmk 2/2 Running 25 (42h ago) 16d Check the Events # Next, we are interested to see all the events on the pod.\nYou can do this by running kubectl describe pod \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; and look at the bottom at the Events. This will give you a lot of information about the pod, including the events that have happened to it similar to the following, which has been redacted to remove sensitive information.\n~ kubectl describe pod my-service-00000000078c9fff-dssbk -n my-service Name: my-service-00000000078c9fff-dssbk Namespace: my-service Priority: 0 Service Account: default Node: \u0026lt;node-details\u0026gt; Start Time: Wed, 25 Jan 2023 15:36:14 +0100 Labels: app.kubernetes.io/instance=my-service app.kubernetes.io/name=my-service pod-template-hash=00000000 Annotations: \u0026lt;annotations\u0026gt; Status: Pending IP: IPs: IP: Controlled By: ReplicaSet/ Containers: my-service: Container ID: Image: \u0026lt;image-name\u0026gt; Image ID: Port: 80/TCP Host Port: 0/TCP State: Waiting Reason: CreateContainerConfigError Ready: False Restart Count: 0 Limits: cpu: 100m memory: 128Mi Requests: cpu: 100m memory: 128Mi Liveness: http-get http://:http/ delay=15s timeout=60s period=60s #success=1 #failure=3 Readiness: http-get http://:http/ delay=15s timeout=60s period=60s #success=1 #failure=3 Environment: (...) AzureWebJobsStorage: \u0026lt;set to the key \u0026#39;AzureWebJobsStorage\u0026#39; in secret \u0026#39;my-service\u0026#39;\u0026gt; Optional: false AzureAccessKey: \u0026lt;set to the key \u0026#39;AzureAccessKey\u0026#39; in secret \u0026#39;my-service\u0026#39;\u0026gt; Optional: false AzureTopicEndpoint: \u0026lt;set to the key \u0026#39;AzureTopicEndpoint\u0026#39; in secret \u0026#39;my-service\u0026#39;\u0026gt; Optional: false ClientId: \u0026lt;set to the key \u0026#39;ClientId\u0026#39; in secret \u0026#39;my-service\u0026#39;\u0026gt; Optional: false (...) State: Waiting (...) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 94s default-scheduler Successfully assigned my-service/my-service-00000000078c9fff-dssbk to \u0026lt;node-name\u0026gt; Normal Pulled 94s kubelet Successfully pulled image \u0026#34;image\u0026#34; in 165.014261ms Warning Failed 77s (x4 over 94s) kubelet Error: couldn\u0026#39;t find key ClientId in Secret my-service/my-service (...) The Events section shows a list of all the events that have occurred in the process of creating the pod.\nAnd here we find the issue. The pod is actually missing a secret, the ClientId in my case, that it needs to start. And that is why the pod is in:\nState: Waiting Reason: CreateContainerConfigError If you want to double-check that the secret is missing, you can run kubectl get secrets -n \u0026lt;namespace\u0026gt; and check if the secret is not there.\nOr you can output it in a JSON format and check that the key is missing by running the following command:\nkubectl get secret my-service -n my-service -o json | jq \u0026#39;.data | map_values(@base64d)\u0026#39; How to Resolve the CreateContainerConfigError # In my case, because I store my infrastructure and configuration (including the kubernetes secrets) in Terraform, I just needed to add the secret to the Terraform configuration, apply it and because the deployment had already timed out, re-run the deployment. But it would\u0026rsquo;ve picked it up automatically if I had applied it a bit sooner.\nNow that the pod has its necessary configuration and is valid if we run kubectl get pods -n \u0026lt;namespace\u0026gt; again, we can see that the pod is now in a Running state.\nkubectl get pods -n my-service NAME READY STATUS RESTARTS AGE my-service-00000000078c9fff-dssbk 2/2 Running 1 (10s ago) 28s my-service-00000000bcddf7d-xfsmk 2/2 Terminating 25 (42h ago) 16d And there you have it. You have successfully resolved the CreateContainerConfigError.\nThis was an easy one, let me know what you encountered in the comments below and how you fixed it.\nHappy Coding and I hope this helps someone!\n","date":"30 January 2023","externalUrl":null,"permalink":"/posts/k8s-pod-createcontainerconfigerror/","section":"Posts","summary":"The other day I was making changes to my helm charts and, after deploying my application, I noticed that one of my pods was stuck in a CreateContainerConfigError state.","title":"Troubleshooting and Resolving a Pod Stuck in 'CreateContainerConfigError' in Kubernetes","type":"posts"},{"content":" For the first book review in what I will call my coffee reads section of the blog I will be reviewing the book Observability Engineering: Achieving Production Excellence.\nObservability Engineering: Achieving Production Excellence # The book, in its own description, sets out to be an advocate for the adoption of observability practices in the software industry. Written by Charity Majors, Liz Fong-Jones, and George Miranda from Honeycomb.io, the book aims to be a resource for anyone interested in learning more about what is good observability, how you can build on top of your system today, and how to implement it in your organization.\nThe book, consisting of around 400 pages, is split into 3 main parts: the first part is an introduction to observability, the second part is a deep dive into the different observability tools and practices, and the third part is a guide on how to implement observability in your organization.\nThoughts on the book # I bought this book for Kindle and went into it with basic knowledge about observability and having used the Honeycomb product a bit for work.\nMy expectations from the title and the summary of the book were to learn about the different tools and practices in the realm of observability in addition to getting a better understanding of the theory behind observability and how it can be implemented in an organization.\nHighlights # The introduction to observability section was a very high-level overview of what observability is and how it differs from monitoring.\nI appreciated the change in mentality the book triggered in me around the concept of observability versus the concept of monitoring. The book did a very good job of explaining how observability has a more holistic approach than monitoring. I could relate to the examples given in the book about how monitoring is a reactive approach to problems and how observability is a proactive approach to problems and all the frustrations that came from relying solely on monitoring such as excessive alerting, false positives, and the lack of context, excessive use of dashboards, and the lack of understanding of the system.\nThe book did not focus solely on the tools that Honeycomb uses and tried to offer an overview of what the market offers for this goal it did a good job at explaining the different tools and practices in terms of observability. The section on tracing explains how tracing works and how it can be used to understand the flow of a request through a system in great detail with examples and code snippets. I found it easy to follow and because of this, I understood what benefits it brings and how it can be used. The section on metrics was also very useful in explaining how you can understand the health of a system by using metrics and how you can also make use of them to detect any system anomalies.\nThe main key takeaway from this chapter was that by using observability concepts and tooling correctly you don\u0026rsquo;t need to rely on software engineers to understand the system, you can use the data, that is available for everyone, to understand the system and make decisions based on the data.\nThe book also emphasizes the organizational and cultural changes that need to be made to implement observability in an organization. As an example, the book explains how the concept of blameless postmortems is a good way to encourage a culture of learning and how it can be used to improve the system. How if you rely on data then all the software engineers with a hero complex will have to adapt. I had a lot of respect for the authors for being honest about these points and for highlighting the day-to-day realities of implementing observability in an organization.\nThe idea of adding case studies of companies that have implemented observability in their organization was very nice and I enjoyed reading about how different companies went about adopting observability practices and the challenges they needed to address.\nAreas for Improvement # Onto the things that I think could be revisited in the next versions of the book.\nThere is a bit of repetition in the book, especially in the first part, where the same concepts are explained in different ways and the same idea, the difference between observability and monitoring, is reiterated several times. Definitely, this could be reduced in the next version of the book.\nThe book also does not go into implementation details on how observability can actually be adopted. It reiterates the challenges but is a bit succinct on how to address them. And the same for implementing observability practices in your system.\nSome sections had a lot of detailed code snippets which were hard to read. I skimmed over them to get the gist while in other sections I would have liked to see more code snippets to help me understand the concepts better.\nThe case study section was too high level for my liking. I would\u0026rsquo;ve loved to read more about the challenges they faced and how they addressed them, rather than mention team collaboration as being the success metric in adopting observability. Go into detail about the tools they used and how they used them. What their lessons learned were.\nSo because of this, the book felt a bit unbalanced at times in my opinion.\nSummary # To summarize, the book, I would say, had a lot of good key takeaways to set you on the path to adopting observability practices either in your day-to-day work or team level or maybe even organizational level, but it did fell short in some areas. Nevertheless, it was a good read and I would recommend it to anyone interested in learning more about observability.\nIf you\u0026rsquo;ve read the book let me know what you thought about it in the comments below. Or if you have any recommendations for other books on the topic of observability I would love to hear them.\nEnjoy your coffee! ☕️\n","date":"24 January 2023","externalUrl":null,"permalink":"/coffee/book-review-observability/","section":"Coffee","summary":"For the first book review in what I will call my coffee reads section of the blog I will be reviewing the book Observability Engineering: Achieving Production Excellence.","title":"Book Review: Observability Engineering: Achieving Production Excellence","type":"coffee"},{"content":" For the first book review in what I will call my coffee reads section of the blog I will be reviewing the book Observability Engineering: Achieving Production Excellence.\nObservability Engineering: Achieving Production Excellence # The book, in its own description, sets out to be an advocate for the adoption of observability practices in the software industry. Written by Charity Majors, Liz Fong-Jones, and George Miranda from Honeycomb.io, the book aims to be a resource for anyone interested in learning more about what is good observability, how you can build on top of your system today, and how to implement it in your organization.\nThe book, consisting of around 400 pages, is split into 3 main parts: the first part is an introduction to observability, the second part is a deep dive into the different observability tools and practices, and the third part is a guide on how to implement observability in your organization.\nThoughts on the book # I bought this book for Kindle and went into it with basic knowledge about observability and having used the Honeycomb product a bit for work.\nMy expectations from the title and the summary of the book were to learn about the different tools and practices in the realm of observability in addition to getting a better understanding of the theory behind observability and how it can be implemented in an organization.\nHighlights # The introduction to observability section was a very high-level overview of what observability is and how it differs from monitoring.\nI appreciated the change in mentality the book triggered in me around the concept of observability versus the concept of monitoring. The book did a very good job of explaining how observability has a more holistic approach than monitoring. I could relate to the examples given in the book about how monitoring is a reactive approach to problems and how observability is a proactive approach to problems and all the frustrations that came from relying solely on monitoring such as excessive alerting, false positives, and the lack of context, excessive use of dashboards, and the lack of understanding of the system.\nThe book did not focus solely on the tools that Honeycomb uses and tried to offer an overview of what the market offers for this goal it did a good job at explaining the different tools and practices in terms of observability. The section on tracing explains how tracing works and how it can be used to understand the flow of a request through a system in great detail with examples and code snippets. I found it easy to follow and because of this, I understood what benefits it brings and how it can be used. The section on metrics was also very useful in explaining how you can understand the health of a system by using metrics and how you can also make use of them to detect any system anomalies.\nThe main key takeaway from this chapter was that by using observability concepts and tooling correctly you don\u0026rsquo;t need to rely on software engineers to understand the system, you can use the data, that is available for everyone, to understand the system and make decisions based on the data.\nThe book also emphasizes the organizational and cultural changes that need to be made to implement observability in an organization. As an example, the book explains how the concept of blameless postmortems is a good way to encourage a culture of learning and how it can be used to improve the system. How if you rely on data then all the software engineers with a hero complex will have to adapt. I had a lot of respect for the authors for being honest about these points and for highlighting the day-to-day realities of implementing observability in an organization.\nThe idea of adding case studies of companies that have implemented observability in their organization was very nice and I enjoyed reading about how different companies went about adopting observability practices and the challenges they needed to address.\nAreas for Improvement # Onto the things that I think could be revisited in the next versions of the book.\nThere is a bit of repetition in the book, especially in the first part, where the same concepts are explained in different ways and the same idea, the difference between observability and monitoring, is reiterated several times. Definitely, this could be reduced in the next version of the book.\nThe book also does not go into implementation details on how observability can actually be adopted. It reiterates the challenges but is a bit succinct on how to address them. And the same for implementing observability practices in your system.\nSome sections had a lot of detailed code snippets which were hard to read. I skimmed over them to get the gist while in other sections I would have liked to see more code snippets to help me understand the concepts better.\nThe case study section was too high level for my liking. I would\u0026rsquo;ve loved to read more about the challenges they faced and how they addressed them, rather than mention team collaboration as being the success metric in adopting observability. Go into detail about the tools they used and how they used them. What their lessons learned were.\nSo because of this, the book felt a bit unbalanced at times in my opinion.\nSummary # To summarize, the book, I would say, had a lot of good key takeaways to set you on the path to adopting observability practices either in your day-to-day work or team level or maybe even organizational level, but it did fell short in some areas. Nevertheless, it was a good read and I would recommend it to anyone interested in learning more about observability.\nIf you\u0026rsquo;ve read the book let me know what you thought about it in the comments below. Or if you have any recommendations for other books on the topic of observability I would love to hear them.\nEnjoy your coffee! ☕️\n","date":"24 January 2023","externalUrl":null,"permalink":"/reads/book-review-observability/","section":"Reads","summary":"For the first book review in what I will call my coffee reads section of the blog I will be reviewing the book Observability Engineering: Achieving Production Excellence.","title":"Book Review: Observability Engineering: Achieving Production Excellence","type":"reads"},{"content":"","date":"24 January 2023","externalUrl":null,"permalink":"/tags/observability/","section":"Tags","summary":"","title":"Observability","type":"tags"},{"content":"In today\u0026rsquo;s world, security is a top priority for any organization or at least it should be. With the rise of cloud computing, the number of security threats has increased exponentially.\nSo how do we keep up? Where do we start?\nMicrosoft has created a set of security benchmarks to give users a starting point for setting up their security configurations. The Microsoft cloud security benchmark (MCSB) is the successor of Azure Security Benchmark (ASB), which was rebranded in October 2022 (Currently in public preview).\nIn this post, I would like to go over the Azure security baseline for Azure Kubernetes Service and give a shoutout to two tools that can aid you in the process of establishing your compliance with the baseline.\nAzure Security Baseline for AKS # The Azure Security Baseline for Azure Kubernetes Service (AKS) is a set of recommendations for securing your AKS cluster.\nIt is an exhaustive list of various aspects of AKS security and it also provides the corresponding actions to be taken in each case. From the documentation\u0026rsquo;s overview:\nYou can monitor this security baseline and its recommendations using Microsoft Defender for Cloud. Azure Policy definitions will be listed in the Regulatory Compliance section of the Microsoft Defender for Cloud dashboard.\nWhen a section has relevant Azure Policy Definitions, they are listed in this baseline to help you measure compliance to the Azure Security Benchmark controls and recommendations. Some recommendations may require a paid Microsoft Defender plan to enable certain security scenarios.\nIt is based on the CIS Kubernetes Benchmark and the Azure Security Benchmark v1.0.\nCIS Benchmarks are best practices for the secure configuration of a target system. Available for more than 100 CIS Benchmarks across 25+ vendor product families, CIS Benchmarks are developed through a unique consensus-based process comprised of cybersecurity professionals and subject matter experts around the world. CIS Benchmarks are the only consensus-based, best-practice security configuration guides both developed and accepted by government, business, industry, and academia.\nFor more information on CIS Benchmark please check CIS Benchmark FAQ.\nFor more information on the CIS Benchmark for Kubernetes please check the kubernetes benchmark.\nIn the CIS Benchmark for Kubernetes document, there are instructions for both Master nodes and Worker nodes. But when using AKS we don\u0026rsquo;t have access to the master nodes. In this case, we can make use of the CIS Benchmark document for AKS.\nWhat could we use to help us check our AKS setup against this benchmark?\nWe can start by looking at the Azure Portal and Microsoft Defender for Cloud, checking out CIS compliance with Kube-bench and any configuration mismatches with Popeye. I will go into more detail on the last two tools. But first, let\u0026rsquo;s see what Microsoft Defender for Cloud looks like and what can you get from it.\nMicrosoft Defender for Cloud # As suggested by Microsoft, we can start with Microsoft Defender for Cloud. If you go to Azure Portal and search for Microsoft Defender for Cloud, then filter by \u0026ldquo;Assessed Resources\u0026rdquo;, and select your cluster you will see a list of all the cluster details and Recommendations and the Alerts tab as well.\nLet\u0026rsquo;s take the first recommendation as an example: Azure Kubernetes Service clusters should have Defender profile enabled\nIf you click on it and expand it will give you the following information: You can choose to Exempt it, meaning you have either fixed this issue or you don\u0026rsquo;t want to fix it or Enforce it, meaning you want to enforce this setting by adding it to an Azure Policy definition.\nThere is also a nice description of the issue and suggested remediation steps to take.\nKube-bench # The official repository can be found here with detailed installation instructions.\nkube-bench is a tool that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark.\nThere are multiple ways of running this tool that you can check here.\nSetting it up # To test out this tool, I decided to just apply it to my local cluster so the first thing I did was start my minikube instance and then I ran the following command:\n\u0026gt; minikube start 😄 minikube v1.22.0 on Darwin 12.6.2 ✨ Using the hyperkit driver based on existing profile 👍 Starting control plane node minikube in cluster minikube 🏃 Updating the running hyperkit \u0026#34;minikube\u0026#34; VM ... 🎉 minikube 1.28.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.28.0 💡 To disable this notice, run: \u0026#39;minikube config set WantUpdateNotification false\u0026#39; 🐳 Preparing Kubernetes v1.21.2 on Docker 20.10.6 ... 🔎 Verifying Kubernetes components... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 🌟 Enabled addons: storage-provisioner, default-storageclass ❗ /usr/local/bin/kubectl is version 1.25.2, which may have incompatibilites with Kubernetes 1.21.2. ▪ Want kubectl v1.21.2? Try \u0026#39;minikube kubectl -- get pods -A\u0026#39; 🏄 Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; cluster and \u0026#34;default\u0026#34; namespace by default # Download the job.yaml file \u0026gt; curl https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml \u0026gt; job.yaml \u0026gt; kubectl apply -f job.yaml job.batch/kube-bench created \u0026gt; kubectl get pods -A  ✔  at minikube ⎈ NAMESPACE NAME READY STATUS RESTARTS AGE default kube-bench-t2fgh 0/1 ContainerCreating 0 5s \u0026gt; kubectl get pods -A  ✔  at minikube ⎈ NAMESPACE NAME READY STATUS RESTARTS AGE default kube-bench-t2fgh 0/1 Completed 0 32s You can run Kube-bench inside a pod, but it will need access to the host\u0026rsquo;s PID namespace to check the running processes, as well as access to some directories on the host where config files and other files are stored.\nThe supplied job.yaml file can be applied to run the tests as a job. This was enough for me to run locally to get a feel of what the tool does and how it generates the report.\nNext, after having run the tests, I wanted to get the report. The results of the tests can be found in the logs of the pod which you can get by running:\n\u0026gt; kubectl logs kube-bench-t2fgh Kube-bench generates a report that looks like the following:\nClick here to expand report [INFO] 1 Master Node Security Configuration [INFO] 1.1 Master Node Configuration Files [PASS] 1.1.1 Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Automated) [PASS] 1.1.2 Ensure that the API server pod specification file ownership is set to root:root (Automated) [PASS] 1.1.3 Ensure that the controller manager pod specification file permissions are set to 644 or more restrictive (Automated) [PASS] 1.1.4 Ensure that the controller manager pod specification file ownership is set to root:root (Automated) [PASS] 1.1.5 Ensure that the scheduler pod specification file permissions are set to 644 or more restrictive (Automated) [PASS] 1.1.6 Ensure that the scheduler pod specification file ownership is set to root:root (Automated) [PASS] 1.1.7 Ensure that the etcd pod specification file permissions are set to 644 or more restrictive (Automated) [PASS] 1.1.8 Ensure that the etcd pod specification file ownership is set to root:root (Automated) [WARN] 1.1.9 Ensure that the Container Network Interface file permissions are set to 644 or more restrictive (Manual) [WARN] 1.1.10 Ensure that the Container Network Interface file ownership is set to root:root (Manual) [FAIL] 1.1.11 Ensure that the etcd data directory permissions are set to 700 or more restrictive (Automated) [FAIL] 1.1.12 Ensure that the etcd data directory ownership is set to etcd:etcd (Automated) [PASS] 1.1.13 Ensure that the admin.conf file permissions are set to 644 or more restrictive (Automated) [PASS] 1.1.14 Ensure that the admin.conf file ownership is set to root:root (Automated) [PASS] 1.1.15 Ensure that the scheduler.conf file permissions are set to 644 or more restrictive (Automated) [PASS] 1.1.16 Ensure that the scheduler.conf file ownership is set to root:root (Automated) [PASS] 1.1.17 Ensure that the controller-manager.conf file permissions are set to 644 or more restrictive (Automated) [PASS] 1.1.18 Ensure that the controller-manager.conf file ownership is set to root:root (Automated) [FAIL] 1.1.19 Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Automated) [WARN] 1.1.20 Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive (Manual) [WARN] 1.1.21 Ensure that the Kubernetes PKI key file permissions are set to 600 (Manual) [INFO] 1.2 API Server [WARN] 1.2.1 Ensure that the --anonymous-auth argument is set to false (Manual) [PASS] 1.2.2 Ensure that the --token-auth-file parameter is not set (Automated) [PASS] 1.2.3 Ensure that the --kubelet-https argument is set to true (Automated) [PASS] 1.2.4 Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate (Automated) [FAIL] 1.2.5 Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated) [PASS] 1.2.6 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated) [PASS] 1.2.7 Ensure that the --authorization-mode argument includes Node (Automated) [PASS] 1.2.8 Ensure that the --authorization-mode argument includes RBAC (Automated) [WARN] 1.2.9 Ensure that the admission control plugin EventRateLimit is set (Manual) [PASS] 1.2.10 Ensure that the admission control plugin AlwaysAdmit is not set (Automated) [WARN] 1.2.11 Ensure that the admission control plugin AlwaysPullImages is set (Manual) [WARN] 1.2.12 Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used (Manual) [PASS] 1.2.13 Ensure that the admission control plugin ServiceAccount is set (Automated) [PASS] 1.2.14 Ensure that the admission control plugin NamespaceLifecycle is set (Automated) [FAIL] 1.2.15 Ensure that the admission control plugin PodSecurityPolicy is set (Automated) [PASS] 1.2.16 Ensure that the admission control plugin NodeRestriction is set (Automated) [PASS] 1.2.17 Ensure that the --insecure-bind-address argument is not set (Automated) [PASS] 1.2.18 Ensure that the --insecure-port argument is set to 0 (Automated) [PASS] 1.2.19 Ensure that the --secure-port argument is not set to 0 (Automated) [FAIL] 1.2.20 Ensure that the --profiling argument is set to false (Automated) [FAIL] 1.2.21 Ensure that the --audit-log-path argument is set (Automated) [FAIL] 1.2.22 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated) [FAIL] 1.2.23 Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated) [FAIL] 1.2.24 Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated) [WARN] 1.2.25 Ensure that the --request-timeout argument is set as appropriate (Manual) [PASS] 1.2.26 Ensure that the --service-account-lookup argument is set to true (Automated) [PASS] 1.2.27 Ensure that the --service-account-key-file argument is set as appropriate (Automated) [PASS] 1.2.28 Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate (Automated) [PASS] 1.2.29 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Automated) [PASS] 1.2.30 Ensure that the --client-ca-file argument is set as appropriate (Automated) [PASS] 1.2.31 Ensure that the --etcd-cafile argument is set as appropriate (Automated) [WARN] 1.2.32 Ensure that the --encryption-provider-config argument is set as appropriate (Manual) [WARN] 1.2.33 Ensure that encryption providers are appropriately configured (Manual) [WARN] 1.2.34 Ensure that the API Server only makes use of Strong Cryptographic Ciphers (Manual) [INFO] 1.3 Controller Manager [WARN] 1.3.1 Ensure that the --terminated-pod-gc-threshold argument is set as appropriate (Manual) [FAIL] 1.3.2 Ensure that the --profiling argument is set to false (Automated) [PASS] 1.3.3 Ensure that the --use-service-account-credentials argument is set to true (Automated) [PASS] 1.3.4 Ensure that the --service-account-private-key-file argument is set as appropriate (Automated) [PASS] 1.3.5 Ensure that the --root-ca-file argument is set as appropriate (Automated) [PASS] 1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true (Automated) [PASS] 1.3.7 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated) [INFO] 1.4 Scheduler [FAIL] 1.4.1 Ensure that the --profiling argument is set to false (Automated) [PASS] 1.4.2 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated) == Remediations master == 1.1.9 Run the below command (based on the file location on your system) on the master node. For example, chmod 644 \u0026lt;path/to/cni/files\u0026gt; 1.1.10 Run the below command (based on the file location on your system) on the master node. For example, chown root:root \u0026lt;path/to/cni/files\u0026gt; 1.1.11 On the etcd server node, get the etcd data directory, passed as an argument --data-dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chmod 700 /var/lib/etcd 1.1.12 On the etcd server node, get the etcd data directory, passed as an argument --data-dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chown etcd:etcd /var/lib/etcd 1.1.19 Run the below command (based on the file location on your system) on the master node. For example, chown -R root:root /etc/kubernetes/pki/ 1.2.5 Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority. --kubelet-certificate-authority=\u0026lt;ca-string\u0026gt; 1.2.9 Follow the Kubernetes documentation and set the desired limits in a configuration file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml and set the below parameters. --enable-admission-plugins=...,EventRateLimit,... --admission-control-config-file=\u0026lt;path/to/configuration/file\u0026gt; 1.2.11 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --enable-admission-plugins parameter to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,... 1.2.12 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --enable-admission-plugins parameter to include SecurityContextDeny, unless PodSecurityPolicy is already in place. --enable-admission-plugins=...,SecurityContextDeny,... 1.2.15 Follow the documentation and create Pod Security Policy objects as per your environment. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --enable-admission-plugins parameter to a value that includes PodSecurityPolicy: --enable-admission-plugins=...,PodSecurityPolicy,... Then restart the API Server. 1.2.20 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the below parameter. --profiling=false 1.2.21 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --audit-log-path parameter to a suitable path and file where you would like audit logs to be written, for example: --audit-log-path=/var/log/apiserver/audit.log 1.2.22 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --audit-log-maxage parameter to 30 or as an appropriate number of days: --audit-log-maxage=30 1.2.23 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --audit-log-maxbackup parameter to 10 or to an appropriate value. --audit-log-maxbackup=10 1.2.24 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --audit-log-maxsize parameter to an appropriate size in MB. For example, to set it as 100 MB: --audit-log-maxsize=100 1.2.25 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml and set the below parameter as appropriate and if needed. For example, --request-timeout=300s 1.2.32 Follow the Kubernetes documentation and configure a EncryptionConfig file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --encryption-provider-config parameter to the path of that file: --encryption-provider-config=\u0026lt;/path/to/EncryptionConfig/File\u0026gt; 1.2.33 Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc, kms or secretbox as the encryption provider. 1.2.34 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the below parameter. --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM _SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM _SHA384 1.3.1 Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the master node and set the --terminated-pod-gc-threshold to an appropriate threshold, for example: --terminated-pod-gc-threshold=10 1.3.2 Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the master node and set the below parameter. --profiling=false 1.4.1 Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.yaml file on the master node and set the below parameter. --profiling=false == Summary master == 39 checks PASS 12 checks FAIL 13 checks WARN 0 checks INFO [INFO] 2 Etcd Node Configuration [INFO] 2 Etcd Node Configuration Files [PASS] 2.1 Ensure that the --cert-file and --key-file arguments are set as appropriate (Automated) [PASS] 2.2 Ensure that the --client-cert-auth argument is set to true (Automated) [PASS] 2.3 Ensure that the --auto-tls argument is not set to true (Automated) [PASS] 2.4 Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate (Automated) [PASS] 2.5 Ensure that the --peer-client-cert-auth argument is set to true (Automated) [PASS] 2.6 Ensure that the --peer-auto-tls argument is not set to true (Automated) [PASS] 2.7 Ensure that a unique Certificate Authority is used for etcd (Manual) == Summary etcd == 7 checks PASS 0 checks FAIL 0 checks WARN 0 checks INFO [INFO] 3 Control Plane Configuration [INFO] 3.1 Authentication and Authorization [WARN] 3.1.1 Client certificate authentication should not be used for users (Manual) [INFO] 3.2 Logging [WARN] 3.2.1 Ensure that a minimal audit policy is created (Manual) [WARN] 3.2.2 Ensure that the audit policy covers key security concerns (Manual) == Remediations controlplane == 3.1.1 Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. 3.2.1 Create an audit policy file for your cluster. 3.2.2 Consider modification of the audit policy in use on the cluster to include these items, at a minimum. == Summary controlplane == 0 checks PASS 0 checks FAIL 3 checks WARN 0 checks INFO [INFO] 4 Worker Node Security Configuration [INFO] 4.1 Worker Node Configuration Files [PASS] 4.1.1 Ensure that the kubelet service file permissions are set to 644 or more restrictive (Automated) [PASS] 4.1.2 Ensure that the kubelet service file ownership is set to root:root (Automated) [PASS] 4.1.3 If proxy kubeconfig file exists ensure permissions are set to 644 or more restrictive (Manual) [PASS] 4.1.4 If proxy kubeconfig file exists ensure ownership is set to root:root (Manual) [PASS] 4.1.5 Ensure that the --kubeconfig kubelet.conf file permissions are set to 644 or more restrictive (Automated) [PASS] 4.1.6 Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root (Automated) [WARN] 4.1.7 Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Manual) [WARN] 4.1.8 Ensure that the client certificate authorities file ownership is set to root:root (Manual) [PASS] 4.1.9 Ensure that the kubelet --config configuration file has permissions set to 644 or more restrictive (Automated) [PASS] 4.1.10 Ensure that the kubelet --config configuration file ownership is set to root:root (Automated) [INFO] 4.2 Kubelet [PASS] 4.2.1 Ensure that the anonymous-auth argument is set to false (Automated) [PASS] 4.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated) [PASS] 4.2.3 Ensure that the --client-ca-file argument is set as appropriate (Automated) [PASS] 4.2.4 Ensure that the --read-only-port argument is set to 0 (Manual) [PASS] 4.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Manual) [FAIL] 4.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Automated) [PASS] 4.2.7 Ensure that the --make-iptables-util-chains argument is set to true (Automated) [WARN] 4.2.8 Ensure that the --hostname-override argument is not set (Manual) [WARN] 4.2.9 Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Manual) [WARN] 4.2.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual) [PASS] 4.2.11 Ensure that the --rotate-certificates argument is not set to false (Automated) [PASS] 4.2.12 Verify that the RotateKubeletServerCertificate argument is set to true (Manual) [WARN] 4.2.13 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual) == Remediations node == 4.1.7 Run the following command to modify the file permissions of the --client-ca-file chmod 644 \u0026lt;filename\u0026gt; 4.1.8 Run the following command to modify the ownership of the --client-ca-file. chown root:root \u0026lt;filename\u0026gt; 4.2.6 If using a Kubelet config file, edit the file to set protectKernelDefaults: true. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --protect-kernel-defaults=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service 4.2.8 Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and remove the --hostname-override argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service 4.2.9 If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service 4.2.10 If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameters in KUBELET_CERTIFICATE_ARGS variable. --tls-cert-file=\u0026lt;path/to/tls-certificate-file\u0026gt; --tls-private-key-file=\u0026lt;path/to/tls-key-file\u0026gt; Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service 4.2.13 If using a Kubelet config file, edit the file to set TLSCipherSuites: to TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 or to a subset of these values. If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the --tls-cipher-suites parameter as follows, or to a subset of these values. --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service == Summary node == 16 checks PASS 1 checks FAIL 6 checks WARN 0 checks INFO [INFO] 5 Kubernetes Policies [INFO] 5.1 RBAC and Service Accounts [WARN] 5.1.1 Ensure that the cluster-admin role is only used where required (Manual) [WARN] 5.1.2 Minimize access to secrets (Manual) [WARN] 5.1.3 Minimize wildcard use in Roles and ClusterRoles (Manual) [WARN] 5.1.4 Minimize access to create pods (Manual) [WARN] 5.1.5 Ensure that default service accounts are not actively used. (Manual) [WARN] 5.1.6 Ensure that Service Account Tokens are only mounted where necessary (Manual) [WARN] 5.1.7 Avoid use of system:masters group (Manual) [WARN] 5.1.8 Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster (Manual) [INFO] 5.2 Pod Security Policies [WARN] 5.2.1 Minimize the admission of privileged containers (Automated) [WARN] 5.2.2 Minimize the admission of containers wishing to share the host process ID namespace (Automated) [WARN] 5.2.3 Minimize the admission of containers wishing to share the host IPC namespace (Automated) [WARN] 5.2.4 Minimize the admission of containers wishing to share the host network namespace (Automated) [WARN] 5.2.5 Minimize the admission of containers with allowPrivilegeEscalation (Automated) [WARN] 5.2.6 Minimize the admission of root containers (Automated) [WARN] 5.2.7 Minimize the admission of containers with the NET_RAW capability (Automated) [WARN] 5.2.8 Minimize the admission of containers with added capabilities (Automated) [WARN] 5.2.9 Minimize the admission of containers with capabilities assigned (Manual) [INFO] 5.3 Network Policies and CNI [WARN] 5.3.1 Ensure that the CNI in use supports Network Policies (Manual) [WARN] 5.3.2 Ensure that all Namespaces have Network Policies defined (Manual) [INFO] 5.4 Secrets Management [WARN] 5.4.1 Prefer using secrets as files over secrets as environment variables (Manual) [WARN] 5.4.2 Consider external secret storage (Manual) [INFO] 5.5 Extensible Admission Control [WARN] 5.5.1 Configure Image Provenance using ImagePolicyWebhook admission controller (Manual) [INFO] 5.7 General Policies [WARN] 5.7.1 Create administrative boundaries between resources using namespaces (Manual) [WARN] 5.7.2 Ensure that the seccomp profile is set to docker/default in your pod definitions (Manual) [WARN] 5.7.3 Apply Security Context to Your Pods and Containers (Manual) [WARN] 5.7.4 The default namespace should not be used (Manual) == Remediations policies == 5.1.1 Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] 5.1.2 Where possible, remove get, list and watch access to secret objects in the cluster. 5.1.3 Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions. 5.1.4 Where possible, remove create access to pod objects in the cluster. 5.1.5 Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false 5.1.6 Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. 5.1.7 Remove the system:masters group from all users in the cluster. 5.1.8 Where possible, remove the impersonate, bind and escalate rights from subjects. 5.2.1 Create a PSP as described in the Kubernetes documentation, ensuring that the .spec.privileged field is omitted or set to false. 5.2.2 Create a PSP as described in the Kubernetes documentation, ensuring that the .spec.hostPID field is omitted or set to false. 5.2.3 Create a PSP as described in the Kubernetes documentation, ensuring that the .spec.hostIPC field is omitted or set to false. 5.2.4 Create a PSP as described in the Kubernetes documentation, ensuring that the .spec.hostNetwork field is omitted or set to false. 5.2.5 Create a PSP as described in the Kubernetes documentation, ensuring that the .spec.allowPrivilegeEscalation field is omitted or set to false. 5.2.6 Create a PSP as described in the Kubernetes documentation, ensuring that the .spec.runAsUser.rule is set to either MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0. 5.2.7 Create a PSP as described in the Kubernetes documentation, ensuring that the .spec.requiredDropCapabilities is set to include either NET_RAW or ALL. 5.2.8 Ensure that allowedCapabilities is not present in PSPs for the cluster unless it is set to an empty array. 5.2.9 Review the use of capabilites in applications running on your cluster. Where a namespace contains applicaions which do not require any Linux capabities to operate consider adding a PSP which forbids the admission of containers which do not drop all capabilities. 5.3.1 If the CNI plugin in use does not support network policies, consideration should be given to making use of a different plugin, or finding an alternate mechanism for restricting traffic in the Kubernetes cluster. 5.3.2 Follow the documentation and create NetworkPolicy objects as you need them. 5.4.1 if possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. 5.4.2 Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. 5.5.1 Follow the Kubernetes documentation and setup image provenance. 5.7.1 Follow the documentation and create namespaces for objects in your deployment as you need them. 5.7.2 Use security context to enable the docker/default seccomp profile in your pod definitions. An example is as below: securityContext: seccompProfile: type: RuntimeDefault 5.7.3 Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers. 5.7.4 Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. == Summary policies == 0 checks PASS 0 checks FAIL 26 checks WARN 0 checks INFO == Summary total == 62 checks PASS 13 checks FAIL 48 checks WARN 0 checks INFO This can also be run inside the AKS cluster by following the instructions here. As a reminder: Kube-bench cannot be run on AKS master nodes. It can only be run on worker nodes, this is not a limitation of Kube-bench but of AKS as mentioned before.\nThe report breakdown # From the report above you can see that Kube-bench benchmarks 5 sections of your configurations which are the following:\nControl Plane Components Etcd Control Plane Configurations Worker Nodes Policies Each section starts by describing which section it targets, the lines having the INFO tag. For example:\n[INFO] 5 Kubernetes Policies [INFO] 5.1 RBAC and Service Accounts Then it lists the checks that are performed for that section. Each check gets a PASS, FAIL or WARN status. For example:\n[WARN] 5.1.1 Ensure that the cluster-admin role is only used where required (Manual) And after the tests run, it also suggests remediations for the check that got a WARN/FAIL status. For example:\n== Remediations policies == 5.1.1 Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] And at the end you can find a summary of the section:\n== Summary policies == 0 checks PASS 0 checks FAIL 26 checks WARN 0 checks INFO Potential use cases # By running it as a CronJon in your cluster, Kube-bench can help you identify potential security issues in your cluster. It is a great tool to have in your toolbox and it is very easy to use. You can configure it to run on a schedule like every week or month and get a report on the security of your cluster, while also taking into account the specific CIS benchmark for your cloud provider. For example, you can set up and run the job-aks.yaml file to run the tests on an AKS cluster.\nPopeye - A Kubernetes Cluster Sanitizer # The repository for the tool can be found here.\nThis is a read-only utility that scans live Kubernetes clusters and reports potential issues with deployed resources and configurations. What I liked about this tool is that it is very easy to install and use and it achieves what it promises: it reduces the cognitive overload one faces when operating a Kubernetes cluster in the wild.\nSetting it up # Popeye can be used standalone using the command line, using a spinach.yml file or running directly in the cluster as a CronJob.\nIn this post, I will be using the command line option on a mac. So to install it I just ran:\n# Install popeye \u0026gt; brew install derailed/popeye/popeye # Check popeye version \u0026gt; popeye version ___ ___ _____ _____ K .-\u0026#39;-. | _ \\___| _ \\ __\\ \\ / / __| 8 __| `\\ | _/ _ \\ _/ _| \\ V /| _| s `-,-`--._ `\\ |_| \\___/_| |___| |_| |___| [] .-\u0026gt;\u0026#39; a `|-\u0026#39; Biffs`em and Buffs`em! `=/ (__/_ / \\_, ` _) `----; | Version: 0.10.1 Commit: ae19897a4b5d3738a3e98179207759e45a53a64c Date: 2022-06-28T14:46:13Z Logs: /var/folders/vp/l8dlq0gn3x71f3vk82shmzlm0000gn/T/popeye.log # Connected to my AKS cluster # Check the context I am using \u0026gt; kubectl config current-context # Run popeye \u0026gt; popeye The report breakdown # The report will be printed to the console and it will look something like the following snippet below. I have removed some of the output for brevity and to give you an idea of the output format and the types of checks that are performed and the results.\nThe report is nicely split into sections and each section has a summary of the checks performed and the results. It ends with giving a grade to the cluster.\nThe color coding is also very helpful to quickly identify the issues:\nLevel Icon Jurassic Color Description Ok ✅ OK Green Happy! Info 🔊 I BlueGreen FYI Warn 😱 W Yellow Potential Issue Error 💥 E Red Action required Click here to expand report \u0026gt; popeye ___ ___ _____ _____ K .-\u0026#39;-. | _ \\___| _ \\ __\\ \\ / / __| 8 __| `\\ | _/ _ \\ _/ _| \\ V /| _| s `-,-`--._ `\\ |_| \\___/_| |___| |_| |___| [] .-\u0026gt;\u0026#39; a `|-\u0026#39; Biffs`em and Buffs`em! `=/ (__/_ / \\_, ` _) `----; | GENERAL [AKS-STAGING] ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · Connectivity...................................................................................✅ · MetricServer...................................................................................✅ CLUSTER (1 SCANNED) 💥 0 😱 0 🔊 0 ✅ 1 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · Version........................................................................................✅ ✅ [POP-406] K8s version OK. CLUSTERROLES (13 SCANNED) 💥 0 😱 0 🔊 0 ✅ 13 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · azure-policy-webhook-cluster-role..............................................................✅ · dapr-operator-admin............................................................................✅ · dashboard-reader...............................................................................✅ · gatekeeper-manager-role........................................................................✅ · grafana-agent..................................................................................✅ · keda-operator..................................................................................✅ · keda-operator-external-metrics-reader..........................................................✅ · kong-kong......................................................................................✅ · omsagent-reader................................................................................✅ · policy-agent...................................................................................✅ · system:coredns-autoscaler......................................................................✅ · system:metrics-server..........................................................................✅ · system:prometheus..............................................................................✅ CLUSTERROLEBINDINGS (19 SCANNED) 💥 0 😱 6 🔊 0 ✅ 13 68٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · azure-policy-webhook-cluster-rolebinding.......................................................✅ · dapr-operator..................................................................................✅ · dapr-role-tokenreview-binding..................................................................😱 😱 [POP-1300] References a ClusterRole (system:auth-delegator) which does not exist. · dashboard-reader-global........................................................................✅ · gatekeeper-manager-rolebinding.................................................................✅ · grafana-agent..................................................................................✅ · keda-operator..................................................................................✅ · keda-operator-hpa-controller-external-metrics..................................................✅ · keda-operator-system-auth-delegator............................................................😱 😱 [POP-1300] References a ClusterRole (system:auth-delegator) which does not exist. · kong-kong......................................................................................✅ · kubelet-api-admin..............................................................................😱 😱 [POP-1300] References a ClusterRole (system:kubelet-api-admin) which does not exist. · metrics-server:system:auth-delegator...........................................................😱 😱 [POP-1300] References a ClusterRole (system:auth-delegator) which does not exist. · omsagentclusterrolebinding.....................................................................✅ · policy-agent...................................................................................✅ · replicaset-controller..........................................................................😱 😱 [POP-1300] References a ClusterRole (system:controller:replicaset-controller) which does not exist. · system:coredns-autoscaler......................................................................✅ · system:discovery...............................................................................😱 😱 [POP-1300] References a ClusterRole (system:discovery) which does not exist. · system:metrics-server..........................................................................✅ · system:prometheus..............................................................................✅ CONFIGMAPS (43 SCANNED) 💥 0 😱 0 🔊 37 ✅ 6 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/kube-root-ca.crt...................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · dapr-system/operator.dapr.io...................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · dapr-system/webhooks.dapr.io...................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · default/kube-root-ca.crt.......................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. (...) DAEMONSETS (9 SCANNED) 💥 0 😱 2 🔊 0 ✅ 7 77٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · kube-system/azure-ip-masq-agent................................................................✅ · kube-system/cloud-node-manager.................................................................✅ · kube-system/cloud-node-manager-windows.........................................................✅ · kube-system/csi-azuredisk-node.................................................................✅ · kube-system/csi-azuredisk-node-win.............................................................✅ · kube-system/csi-azurefile-node.................................................................✅ · kube-system/csi-azurefile-node-win.............................................................✅ · kube-system/kube-proxy.........................................................................😱 🐳 kube-proxy 😱 [POP-107] No resource limits defined. 🐳 kube-proxy-bootstrap 😱 [POP-107] No resource limits defined. · prometheus-agent/grafana-agent.................................................................😱 🐳 agent 😱 [POP-106] No resources requests/limits defined. DEPLOYMENTS (29 SCANNED) 💥 0 😱 4 🔊 3 ✅ 22 86٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/dapr-dashboard.....................................................................🔊 🐳 dapr-dashboard 🔊 [POP-108] Unnamed port 8080. · dapr-system/dapr-operator......................................................................🔊 🐳 dapr-operator 🔊 [POP-108] Unnamed port 6500. · dapr-system/dapr-sentry........................................................................🔊 🐳 dapr-sentry 🔊 [POP-108] Unnamed port 50001. · dapr-system/dapr-sidecar-injector..............................................................✅ · gatekeeper-system/gatekeeper-audit.............................................................✅ · gatekeeper-system/gatekeeper-controller........................................................✅ · keda-system/keda-operator......................................................................✅ · keda-system/keda-operator-metrics-apiserver....................................................✅ · kong/kong-kong.................................................................................😱 🐳 ingress-controller 😱 [POP-106] No resources requests/limits defined. 🐳 proxy 😱 [POP-106] No resources requests/limits defined. · kube-system/azure-policy.......................................................................✅ · kube-system/azure-policy-webhook...............................................................✅ · kube-system/coredns............................................................................✅ · kube-system/coredns-autoscaler.................................................................✅ · kube-system/konnectivity-agent.................................................................✅ · kube-system/metrics-server.....................................................................✅ (...) HORIZONTALPODAUTOSCALERS (4 SCANNED) 💥 0 😱 2 🔊 0 ✅ 2 50٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · HPA............................................................................................😱 😱 [POP-604] If ALL HPAs triggered, 11500m will match/exceed cluster CPU(4931m) capacity by 6570m. 😱 [POP-605] If ALL HPAs triggered, 14720Mi will match/exceed cluster memory(1523Mi) capacity by 13196Mi. · my-service/keda-hpa-my-service.................................................................✅ · my-service-2/keda-hpa-my-service-2.............................................................😱 😱 [POP-602] Replicas (1/100) at burst will match/exceed cluster CPU(4931m) capacity by 5070m. 😱 [POP-603] Replicas (1/100) at burst will match/exceed cluster memory(1523Mi) capacity by 11276Mi. (...) INGRESSES (13 SCANNED) 💥 0 😱 0 🔊 0 ✅ 13 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · my-service/my-service..........................................................................✅ · kong/kong-dapr.................................................................................✅ (...) NAMESPACES (23 SCANNED) 💥 0 😱 0 🔊 3 ✅ 20 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system....................................................................................✅ · default........................................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · gatekeeper-system..............................................................................✅ · keda-system....................................................................................✅ · kong...........................................................................................✅ · kube-node-lease................................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · kube-public....................................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · kube-system....................................................................................✅ · logstash.......................................................................................✅ · prometheus-agent...............................................................................✅ (...) NETWORKPOLICIES (2 SCANNED) 💥 0 😱 0 🔊 0 ✅ 2 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · kube-system/konnectivity-agent.................................................................✅ · kube-system/tunnelfront........................................................................✅ NODES (3 SCANNED) 💥 0 😱 2 🔊 0 ✅ 1 33٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · aks-default-***-vmss000000.....................................................................😱 😱 [POP-710] Memory threshold (80%) reached 83%. · aks-default-***-vmss000001.....................................................................😱 😱 [POP-710] Memory threshold (80%) reached 105%. · aks-default-***-vmss00000a.....................................................................✅ PERSISTENTVOLUMES (3 SCANNED) 💥 0 😱 0 🔊 0 ✅ 3 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · pvc-**.........................................................................................✅ · pvc-**.........................................................................................✅ · pvc-**.........................................................................................✅ PERSISTENTVOLUMECLAIMS (3 SCANNED) 💥 0 😱 0 🔊 0 ✅ 3 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/raft-log-dapr-placement-server-0...................................................✅ · dapr-system/raft-log-dapr-placement-server-1...................................................✅ · dapr-system/raft-log-dapr-placement-server-2...................................................✅ PODS (72 SCANNED) 💥 1 😱 71 🔊 0 ✅ 0 0٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/dapr-dashboard-1-lq942.............................................................😱 😱 [POP-301] Connects to API Server? ServiceAccount token is mounted. 🐳 dapr-dashboard 😱 [POP-102] No probes defined. 🔊 [POP-108] Unnamed port 8080. · dapr-system/dapr-operator-1-chmmq..............................................................😱 😱 [POP-301] Connects to API Server? ServiceAccount token is mounted. 🐳 dapr-operator 😱 [POP-205] Pod was restarted (17) times. 🔊 [POP-105] Liveness probe uses a port#, prefer a named port. 🔊 [POP-105] Readiness probe uses a port#, prefer a named port. 🔊 [POP-108] Unnamed port 6500. · dapr-system/dapr-placement-server-0............................................................😱 😱 [POP-301] Connects to API Server? ServiceAccount token is mounted. 😱 [POP-302] Pod could be running as root user. Check SecurityContext/Image. 🐳 dapr-placement-server 🔊 [POP-105] Liveness probe uses a port#, prefer a named port. 🔊 [POP-105] Readiness probe uses a port#, prefer a named port. 😱 [POP-306] Container could be running as root user. Check SecurityContext/Image. (...) PODDISRUPTIONBUDGETS (8 SCANNED) 💥 0 😱 2 🔊 0 ✅ 6 75٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/dapr-dashboard-disruption-budget...................................................✅ · dapr-system/dapr-operator-disruption-budget....................................................✅ · dapr-system/dapr-placement-server-disruption-budget............................................✅ · dapr-system/dapr-sentry-budget.................................................................✅ · dapr-system/dapr-sidecar-injector-disruption-budget............................................✅ · kube-system/coredns-pdb........................................................................😱 😱 [POP-403] Deprecated PodDisruptionBudget API group \u0026#34;policy/v1beta1\u0026#34;. Use \u0026#34;policy/v1\u0026#34; instead. · kube-system/konnectivity-agent.................................................................😱 😱 [POP-403] Deprecated PodDisruptionBudget API group \u0026#34;policy/v1beta1\u0026#34;. Use \u0026#34;policy/v1\u0026#34; instead. · logstash/logstash-logstash-pdb.................................................................✅ PODSECURITYPOLICIES (0 SCANNED) 💥 0 😱 0 🔊 0 ✅ 0 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · Nothing to report. REPLICASETS (197 SCANNED) 💥 0 😱 0 🔊 0 ✅ 197 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/dapr-dashboard-1...................................................................✅ · dapr-system/dapr-operator-1....................................................................✅ · dapr-system/dapr-sentry-1......................................................................✅ · dapr-system/dapr-sidecar-injector-1............................................................✅ · keda-system/keda-operator-1....................................................................✅ · keda-system/keda-operator-metrics-apiserver-1..................................................✅ (...) ROLES (5 SCANNED) 💥 0 😱 0 🔊 0 ✅ 5 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · default/secret-reader..........................................................................✅ · gatekeeper-system/gatekeeper-manager-role......................................................✅ · kong/kong-kong.................................................................................✅ · kube-system/azure-policy-webhook-role..........................................................✅ · kube-system/policy-pod-agent...................................................................✅ ROLEBINDINGS (7 SCANNED) 💥 0 😱 2 🔊 0 ✅ 5 71٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · default/dapr-secret-reader.....................................................................✅ · gatekeeper-system/gatekeeper-manager-rolebinding...............................................✅ · kong/kong-kong.................................................................................✅ · kube-system/azure-policy-webhook-rolebinding...................................................✅ · kube-system/keda-operator-auth-reader..........................................................😱 😱 [POP-1300] References a Role (kube-system/extension-apiserver-authentication-reader) which does not exist. · kube-system/metrics-server-auth-reader.........................................................😱 😱 [POP-1300] References a Role (kube-system/extension-apiserver-authentication-reader) which does not exist. · kube-system/policy-pod-agent...................................................................✅ SECRETS (277 SCANNED) 💥 0 😱 0 🔊 254 ✅ 23 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/dapr-operator-token-tdnb4..........................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · dapr-system/dapr-sidecar-injector-cert.........................................................✅ · dapr-system/dapr-trust-bundle..................................................................✅ (...) SERVICES (35 SCANNED) 💥 3 😱 17 🔊 7 ✅ 8 42٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/dapr-api...........................................................................🔊 🔊 [POP-1102] Use of target port #6500 for service port TCP::80. Prefer named port. · dapr-system/dapr-dashboard.....................................................................😱 🔊 [POP-1102] Use of target port #8080 for service port TCP::8080. Prefer named port. 😱 [POP-1109] Only one Pod associated with this endpoint. · dapr-system/dapr-placement-server..............................................................🔊 🔊 [POP-1102] Use of target port #50005 for service port TCP:api:50005. Prefer named port. 🔊 [POP-1102] Use of target port #8201 for service port TCP:raft-node:8201. Prefer named port. · dapr-system/dapr-sentry........................................................................🔊 🔊 [POP-1102] Use of target port #50001 for service port TCP::80. Prefer named port. · dapr-system/dapr-sidecar-injector..............................................................✅ · dapr-system/dapr-webhook.......................................................................💥 💥 [POP-1106] No target ports match service port TCP::443. · default/dapr-eventgrid-func-dapr...............................................................💥 💥 [POP-1100] No pods match service selector. 💥 [POP-1105] No associated endpoints. · default/kubernetes.............................................................................✅ (...) SERVICEACCOUNTS (38 SCANNED) 💥 0 😱 1 🔊 8 ✅ 29 97٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/dapr-operator......................................................................✅ · dapr-system/dashboard-reader...................................................................✅ · dapr-system/default............................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · default/default................................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · domain-event-emitter/default...................................................................✅ · domain-start/default...........................................................................✅ · gatekeeper-system/default......................................................................🔊 🔊 [POP-400] Used? Unable to locate resource reference. · gatekeeper-system/gatekeeper-admin.............................................................✅ (...) STATEFULSETS (2 SCANNED) 💥 0 😱 0 🔊 0 ✅ 2 100٪ ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ · dapr-system/dapr-placement-server..............................................................✅ · logstash/logstash-logstash.....................................................................✅ SUMMARY ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅ Your cluster score: 82 -- B o .-\u0026#39;-. o __| B `\\ o `-,-`--._ `\\ [] .-\u0026gt;\u0026#39; a `|-\u0026#39; `=/ (__/_ / \\_, ` _) `----; | Popeye has a bit of a different scope than Kube-bench in the sense that it scans your cluster for best practices and potential issues. It is not a security scanner like Kube-bench, but it can be used to find potential security issues and for your cluster management.\nIt targets the following:\nnodes namespaces pods services Its main scope is to find misconfigurations like port mismatches, dead or unused resources etc. You can find the full list of available sanitizers here.\nPotential use cases # Should you choose it to run in a pipeline or as a job you can build on it and make action items to fix based on the errors reported in the report.\nOr this could just be a little helpful tool to run on your cluster to get a quick overview of the state of your cluster. And you can extract any action items for yourself. In this case, you can save the report using the --save flag and attach it to your JIRA Ticket or PR.\nThis will save the report in your current working directory.\n\u0026gt; popeye --save /var/folders/vp/l8dlq0gn3x71f3vk82shmzlm0000gn/T/popeye/sanitizer_aks-staging_1673563387070145000.txt Summary # I quite enjoyed using both Popeye and Kube-bench. They are both very useful in different ways. Popeye is more of a cluster management tool, while Kube-bench is more of a security scanner. But they can both be used to improve the security of your cluster.\nIn the next posts from this series, we will take a deeper look at the reports in more detail, focusing on the errors and seeing what we can do to improve our cluster score.\nUntil then, thank you for reading and I hope you found this useful. Let me know what other tools you use\n","date":"23 January 2023","externalUrl":null,"permalink":"/posts/k8s-aks-security-helpers/","section":"Posts","summary":"In today\u0026rsquo;s world, security is a top priority for any organization or at least it should be.","title":"Kube-bench and Popeye: A Power Duo for AKS Security Compliance","type":"posts"},{"content":" My blog # My name is Ana, and I am a cloud engineer with a passion for technology. I have several years of experience working with cloud computing platforms such as Azure, Infrastructure as Code, and Observability tools and I enjoy sharing my knowledge and experience with others.\nI\u0026rsquo;ll be writing about a wide range of topics related to cloud computing, infrastructure as code, observability, how-tos and lessons learned in my journey and technology in general, including best practices, tips or optimizing cloud costs.\nI\u0026rsquo;ll also be sharing my personal experiences and lessons learned in the software industry, and technical books I\u0026rsquo;m reading or any other topics that I find interesting.\nWhether you\u0026rsquo;re a developer, an IT professional, or just someone who is curious about the tech, I hope you\u0026rsquo;ll find something of interest here. If you have any questions, or feedback or just want to get in touch, please feel free to reach out to me.\nThanks for visiting!\nCertifications # Microsoft Certified: DevOps Engineer Expert Microsoft Certified: Azure Administrator Associate Microsoft Certified: Azure Fundamentals Other projects # Proud member of the Ops community. Give it a look if you\u0026rsquo;re interested in learning more about the Ops community and how you can get involved and contribute to the community.\nTutor for the Udacity Cloud DevOps Nanodegree program. I was a tutor for the Udacity Cloud DevOps Nanodegree program. I\u0026rsquo;m passionate about helping others learn and grow, and I\u0026rsquo;m excited to be able to share my knowledge and experience with others. If you\u0026rsquo;re interested in learning more about the program, please feel free to reach out to me.\n","date":"7 January 2023","externalUrl":null,"permalink":"/about/","section":"Cloudy Caffeine with Ana","summary":"My blog # My name is Ana, and I am a cloud engineer with a passion for technology.","title":"Welcome to Cloudy Caffeine with Ana!","type":"page"},{"content":"","date":"10 August 2022","externalUrl":null,"permalink":"/tags/helm/","section":"Tags","summary":"","title":"Helm","type":"tags"},{"content":"When I started migrating to Kubernetes (K8s) I discovered that I can use Terraform for managing not only the infrastructure, but also I could define the K8s objects in it, but I also could use Helm to handle that. But what would be a good way to handle this?\nIn this post we will cover Terraform and Helm for managing Kubernetes clusters with some code snippets and an idea on how you can use them together to get you started.\nStructure of the post: # What is Terraform? Manage Kubernetes Resources via Terraform What is Helm? Manage Kubernetes Resources via Helm Using Helm and Terraform Together What is Terraform? # HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share.\nIt can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features.\nTerraform treats Infrastructure as Code (IaC) meaning teams manage infrastructure setup with configuration files instead of using graphical user interface (think of Azure Portal and the like).\nSo why would you use Terraform?\nSome of the benefits include:\nIt allows teams to build, change, and manage the infrastructure in a safe, consistent, and repeatable way by defining resource configurations that can be versioned, reused, and shared.\nIt supports all major cloud providers: Azure, AWS, GCP and many other which you can find by browsing their registry.\nProviders define individual units of infrastructure, for example compute instances or private networks, as resources. You can compose resources from different providers into reusable Terraform configurations called modules, and manage them with a consistent language and workflow.\nYou define your providers in the terraform code as follo3ws:\nterraform { required_providers { helm = { version = \u0026#34;2.5.1\u0026#34; } azuread = { source = \u0026#34;hashicorp/azuread\u0026#34; version = \u0026#34;2.23.0\u0026#34; } azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; } } It\u0026rsquo;s configuration language is declarative: Meaning that it describes the desired end-state for your infrastructure, in contrast to procedural programming languages that require step-by-step instructions to perform tasks. Terraform providers automatically calculate dependencies between resources to create or destroy them in the correct order.\nThis means that any new team member joining will be able to understand the infrastructure setup you have just by going through the configuration files.\nIt\u0026rsquo;s state allows you to track resource changes throughout your deployments.\nAll configurations are subject to version control to safely collaborate on infrastructure.\nYou can read more on the advantages it brings by looking over the official use cases from the Terraform documentation.\nTerraform - How does it work?\nTerraform follows a simple workflow for managing your infrastructure. So once a new resource or changes to resources are desired, the team will:\nInitialize the backend by running the terraform init command, which will install the plugins Terraform needs to manage the infrastructure.\nOutput will look something like:\nInitializing modules... (...) - module1 in ../../modules/module1 - module2 in ../../modules/module2 Initializing the backend... Successfully configured the backend \u0026#34;azurerm\u0026#34;! Terraform will automatically use this backend unless the backend configuration changes. Initializing provider plugins... - Reusing previous version of hashicorp/kubernetes from the dependency lock file - Reusing previous version of hashicorp/azuread from the dependency lock file - Reusing previous version of hashicorp/azurerm from the dependency lock file - Reusing previous version of hashicorp/helm from the dependency lock file - Installing hashicorp/helm v2.5.1... - Installed hashicorp/helm v2.5.1 (signed by HashiCorp) - Installing hashicorp/kubernetes v2.10.0... - Installed hashicorp/kubernetes v2.10.0 (signed by HashiCorp) - Installing hashicorp/azuread v2.23.0... - Installed hashicorp/azuread v2.23.0 (signed by HashiCorp) - Installing hashicorp/azurerm v3.10.0... - Installed hashicorp/azurerm v3.10.0 (signed by HashiCorp) Partner and community providers are signed by their developers. If you\u0026#39;d like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. In the example above we initialize the backend, in our case we store the state file in a container in azure, the providers (azurerm, azuread, helm, kubernetes) and we get a successful message of completion.\nPlan the changes to be made by running the terraform plan command, which will give a review of the \u0026lsquo;planned\u0026rsquo; changes Terraform will make to match your configuration.\nDuring the plan, Terraform will mark which resources will be:\nadded with a \u0026lsquo;+\u0026rsquo; sign, updated with a \u0026lsquo;~\u0026rsquo; sign or deleted with a \u0026lsquo;-\u0026rsquo; sign. In this example Terraform is creating a brand new resource as you can see all attributes are marked with the + sign:\n# module.monitoring.helm_release.prometheus_agent[0] will be created + resource \u0026#34;helm_release\u0026#34; \u0026#34;prometheus_agent\u0026#34; { + atomic = false + chart = \u0026#34;../../charts/prometheus-agent\u0026#34; + cleanup_on_fail = false + create_namespace = false + dependency_update = false + disable_crd_hooks = false + disable_openapi_validation = false + disable_webhooks = false + force_update = false (...) But in this one, the resource was already previously created, and Terraform is just updating something to it which is marked with the ~ sign:\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create ~ update in-place Terraform will perform the following actions: # module.grafana.grafana_organization.org will be updated in-place ~ resource \u0026#34;grafana_organization\u0026#34; \u0026#34;org\u0026#34; { ~ admins = [ + \u0026#34;new_admin@grafana.admin\u0026#34;, ] id = \u0026#34;1\u0026#34; name = \u0026#34;MyOrg\u0026#34; # (5 unchanged attributes hidden) } Terraform keeps track of your real infrastructure in a state file, which acts as a source of truth for your environment. Meaning it uses this file to determine the changes to make to your infrastructure so that it will match your configuration.\nThis is also helpful to detect infrastructure drifts between the desired state and the current one.\nApply the desired changes by running the terraform apply command.\nA view from the Terraform official documentation:\nSo this is Terraform in a nutshell.\nManage Kubernetes Resources via Terraform # Terraform’s Kubernetes (K8S) provider is used to interact with the resources supported by Kubernetes and offers many benefits, but it\u0026rsquo;s important to note that the capability is still new. This means you might not have all of the resources available in the provider or there might be some open bugs.\nThat said, how would this look? Let\u0026rsquo;s look at an example for an AKS cluster in Terraform:\nresource \u0026#34;azurerm_kubernetes_cluster\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;aks-${var.prefix}-${var.env}\u0026#34; location = azurerm_resource_group.main.location resource_group_name = azurerm_resource_group.main.name dns_prefix = \u0026#34;${var.prefix}-${var.env}\u0026#34; role_based_access_control_enabled = var.ad_admin_group == \u0026#34;\u0026#34; ? false : true kubernetes_version = var.kubernetes_version dynamic \u0026#34;azure_active_directory_role_based_access_control\u0026#34; { for_each = var.ad_admin_group == \u0026#34;\u0026#34; ? [] : [1] content { admin_group_object_ids = [var.ad_admin_group] azure_rbac_enabled = true } } dynamic \u0026#34;oms_agent\u0026#34; { for_each = var.oms_agent_enabled == true ? [1] : [] content { log_analytics_workspace_id = var.log_analytics_workspace_id } } azure_policy_enabled = var.azure_policy_enabled http_application_routing_enabled = false api_server_authorized_ip_ranges = var.api_server_authorized_ip_ranges default_node_pool { name = \u0026#34;default\u0026#34; enable_auto_scaling = var.enable_auto_scaling max_count = var.enable_auto_scaling ? var.max_count : null min_count = var.enable_auto_scaling ? var.min_count : null node_count = var.node_count type = \u0026#34;VirtualMachineScaleSets\u0026#34; vm_size = var.node_size tags = var.tags orchestrator_version = var.node_pool_orchestrator_version } service_principal { client_id = azuread_application.main.application_id client_secret = azuread_service_principal_password.main.value } tags = var.tags } So you create the resources, run terraform apply and it will provision your infrastructure.\nFor the deployment we create a separate Terraform file using the kubernetes deployment resource:\nresource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;example\u0026#34; { metadata { name = \u0026#34;example\u0026#34; labels = { App = \u0026#34;Example\u0026#34; } } spec { replicas = 2 selector { match_labels = { App = \u0026#34;Example\u0026#34; } } template { metadata { labels = { App = \u0026#34;Example\u0026#34; } } spec { container { image = \u0026#34;nginx:1.7.8\u0026#34; name = \u0026#34;example\u0026#34; port { container_port = 80 } resources { limits = { cpu = \u0026#34;0.5\u0026#34; memory = \u0026#34;512Mi\u0026#34; } requests = { cpu = \u0026#34;250m\u0026#34; memory = \u0026#34;50Mi\u0026#34; } } } } } } } And in order to create this you again run terraform apply and confirm the changes.\nSame approach will apply to creating a service:\nresource \u0026#34;kubernetes_service\u0026#34; \u0026#34;example\u0026#34; { metadata { name = \u0026#34;example\u0026#34; } spec { selector = { App = kubernetes_deployment.example.spec.0.template.0.metadata[0].labels.App } port { port = 80 target_port = 80 } type = \u0026#34;LoadBalancer\u0026#34; } } And if you want to scale this setup then the approach is:\nMake the changes to the replica count spec { replicas = 3 selector { match_labels = { App = \u0026#34;Example\u0026#34; } } Apply the terraform code and confirm the changes Aside from this we can store the kubernetes_namespace resource:\nresource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;example\u0026#34; { metadata { annotations = { name = \u0026#34;example\u0026#34; } name = \u0026#34;example\u0026#34; } } And any secrets you might need:\nresource \u0026#34;kubernetes_secret\u0026#34; \u0026#34;example\u0026#34; { metadata { name = \u0026#34;example\u0026#34; namespace = \u0026#34;example\u0026#34; } data = { \u0026#34;some_setting\u0026#34; = \u0026#34;false\u0026#34; } } Using this approach means you can take advantage of the benefits of Terraform including:\ncan use one tool for managing your infrastructure resources and also for your cluster management you use one language for all your infrastructure resources and also the k8s objects you can nicely see the plan of your changes before provisioning resources The disadvantages are of course you need to be familiar with hcl language and if the team is new adopting it would take a bit of time.\nThe K8s Terraform provider might not fully support all the beta objects so you might need to wait.\nIf you are interested in provisioning a cluster and all the K8s objects via Terraform please check the official documentation for step by step settings.\nWhat is Helm? # Helm is a package manager tool that helps you manage Kubernetes applications. Helm makes use of Helm Charts to define, install, and upgrade Kubernetes application.\nLet\u0026rsquo;s look over some terminology when working with Helm:\nHelm: is the command-line interface that helps you define, install, and upgrade your Kubernetes application using charts.\nCharts: are the format for Helm’s application package. The chart is a bundle of information necessary to create an instance of a Kubernetes application. Basically a package of files and templates that gets converted into Kubernetes objects at deployment time.\nChart repository: is the location where you can store and share packaged charts.\nThe config: contains configuration information that can be merged into a packaged chart to create a releasable object.\nThe Release: is a running instance of a chart, combined with a specific config. It is created by Helm to track the installation of the charts you created/defined.\nFor more details on Helm architecture.\nSome of the benefits Helm brings:\nCharts are in YAML format and are reusable because they provide repeatable application installation. Because of this you can use them in multiple environments (think dev, staging and prod following the same one). Because charts build a repeatable process this makes deployments easier. A lot of charts are already available, but you can create your own as well - custom charts. You can create dependencies between the charts and can also use sub-charts to add more flexibility to your setup. Charts serve as a single point of authority. Releases are tracked. You can upgrade or rollback multiple K8s objects together. Charts can be easily installed/ uninstalled. Manage Kubernetes Resources via Helm # We looked over the main terminology when using Helm, but let\u0026rsquo;s see how it would look like.\nFirst thing we need to do is in the repository where we have our code we run the helm create \u0026lt;app_name\u0026gt; command. This command creates a chart directory along with the common files and directories used in a chart. More information on the command.\nAnd this will create a structure as follows:\n. └── example ├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── hpa.yaml │ ├── ingress.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml We notice it created: A Chart.yaml file which just contains the information about the chart.\napiVersion: v2 name: example description: A Helm chart for Kubernetes # A chart can be either an \u0026#39;application\u0026#39; or a \u0026#39;library\u0026#39; chart. # # Application charts are a collection of templates that can be packaged into versioned archives # to be deployed. # # Library charts provide useful utilities or functions for the chart developer. They\u0026#39;re included as # a dependency of application charts to inject those utilities and functions into the rendering # pipeline. Library charts do not define any templates and therefore cannot be deployed. type: application # This is the chart version. This version number should be incremented each time you make changes # to the chart and its templates, including the app version. # Versions are expected to follow Semantic Versioning (https://semver.org/) version: 0.1.0 # This is the version number of the application being deployed. This version number should be # incremented each time you make changes to the application. Versions are not expected to # follow Semantic Versioning. They should reflect the version the application is using. # It is recommended to use it with quotes. appVersion: \u0026#34;1.16.0\u0026#34; The charts directory where you can add any charts that your chart depends on.\nThe templates directory: A directory to store partials and helpers. The file called _helpers.tpl is the default location for template partials that the rest of the yaml files rely on as we will see.\nHow does this work?\nLet\u0026rsquo;s take a small example, in that file we define the fullname:\n{{/* Create a default fully qualified app name. We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec). If release name contains chart name it will be used as a full name. */}} {{- define \u0026#34;example.fullname\u0026#34; -}} {{- if .Values.fullnameOverride }} {{- .Values.fullnameOverride | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- else }} {{- $name := default .Chart.Name .Values.nameOverride }} {{- if contains $name .Release.Name }} {{- .Release.Name | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- else }} {{- printf \u0026#34;%s-%s\u0026#34; .Release.Name $name | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }} {{- end }} {{- end }} And in our service.yaml file we can include the defined fullname like:\napiVersion: v1 kind: Service metadata: name: {{ include \u0026#34;example.fullname\u0026#34; . }} labels: {{- include \u0026#34;example.labels\u0026#34; . | nindent 4 }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: http protocol: TCP name: http selector: {{- include \u0026#34;example.selectorLabels\u0026#34; . | nindent 4 }} And in the values.yaml file we can also override it:\n# Default values for example. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: nginx pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \u0026#34;\u0026#34; imagePullSecrets: [] nameOverride: \u0026#34;\u0026#34; fullnameOverride: \u0026#34;\u0026#34; serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \u0026#34;\u0026#34; podAnnotations: {} podSecurityContext: {} # fsGroup: 2000 securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 service: type: ClusterIP port: 80 (...) In the example above the fullname will be the Chart name since we didn\u0026rsquo;t pass any value in the values.yaml file for the override based on the definition of the parameter in the _helpers.tpl file.\nThe values.yaml file which contains the default values for your templates. You can at this point split the values file per each environment: one for staging, one for production etc.\nFrom here onwards you can start configuring based on what you need. If you check the yaml files you will notice the structure is very similar to what we defined in the K8s objects terraform code.\nFor the installation of the charts you can either go one by one and make use of the helm install/upgrade commands OR you can add this in your CI/CD pipelines.\nGoing by the command line could look something like: helm upgrade example infra/charts/example --install --wait --atomic --namespace=example --set=app.name=example --values=infra/charts/example/values.yaml where:\ninfra/charts/example - is the location of your Chart.yaml file values=infra/charts/example/values.yaml - is the location of the values file --wait - will wait until either the release is successful or the default timeout is reached (5m) if no timeout is specified --atomic- if set, upgrade process rolls back changes made in case of failed upgrade Check the full synopsis of the command here.\nhelm install or helm upgrade \u0026ndash;install?\nThe install sub-command always installs a brand new chart, while the upgrade sub-command can upgrade an existing chart and install a new one, if the chart hasn\u0026rsquo;t been installed before.\nFor simplicity you can always you the upgrade sub-command.\nUsing Helm and Terraform Together # Helm and Terraform are not mutually exclusive and can be used together in the same K8s setup even if the actual setup really depends on your project complexity, which benefits you want to make use of and which drawbacks you can live with.\nIn a potential setup where you would use both you could structure it something like:\nuse Terraform to create and manage resources: the K8s Cluster, the K8s namespace, and the K8s secrets( if any ) use Helm charts to deploy your applications This is the setup we currently use and it has served us well so far.\nIt is worth mentioning that you can also use Terraform to handle your Helm deploys using the helm_release resource.\nIn this approach you would have both infrastructure and provisioning in one place - in Terraform. I will not go in this post in the differences between them, but I will mention going with this approach should depend on how frequent you need to apply changes to your infrastructure because the way this works is during terraform apply operation the helm release will take place.\nThere is no one-size-fits-all approach, but you should tailor tooling and the strategy to your needs.\nHope you find this helpful. Thank you for reading and feel free to comment on your experience and what you prefer to use and why.\n","date":"10 August 2022","externalUrl":null,"permalink":"/posts/terraform-vs-helm-for-managing-k8s-objects/","section":"Posts","summary":"When I started migrating to Kubernetes (K8s) I discovered that I can use Terraform for managing not only the infrastructure, but also I could define the K8s objects in it, but I also could use Helm to handle that.","title":"Terraform vs Helm for Managing K8s Objects","type":"posts"},{"content":"We ran into an issue while applying our Terraform infrastructure on a M1 Mac where we were making use of the Terraform Provider Template.\nWhen applying it, we were getting the following error:\ntemplate v2.2.0 does not have a package available for your current platform, darwin_arm64 Since the provider is archived, we need to find an alternative.\nWhat does archiving mean? Per Terraform Archiving Providers documentation.\nThe code repository and all commit, issue, and PR history will still be available. Existing released binaries will remain available on the releases site. Documentation for the provider will remain on the Terraform website. Issues and pull requests are not being monitored, merged, or added. No new releases will be published. Nightly acceptance tests may not be run. So what alternatives do we have instead of the deprecated provider?\nLet\u0026rsquo;s look at an example.\nResource using the deprecated Template provider # Let\u0026rsquo;s say we have the following resource - a grafana dashboard json that we store in our Terraform code.\ndata \u0026#34;template_file\u0026#34; \u0026#34;grafana_json\u0026#34; { template = file(\u0026#34;${path.module}/grafana_dashboard.json\u0026#34;) vars = { title = var.monitoring_title monitoring_datasource_name = var.monitoring_datasource_name } } And the grafana dashboard Terraform resource:\nresource \u0026#34;grafana_dashboard\u0026#34; \u0026#34;metrics\u0026#34; { config_json = data.template_file.grafana_json.rendered folder = var.monitoring_folder } When you try to apply this piece of code it will throw the aforementioned error.\nUpdating to the built-in templatefile Terraform function # We can make use of the built-in templatefile Terraform function that:\ntemplatefile reads the file at the given path and renders its content as a template using a supplied set of template variables.\nThe function uses the format: templatefile(path, vars)\nIn our case the path is the path to the grafana json file.\nAnd vars contains all the variables that we need to use for the grafana dashboard json file.\nThe \u0026ldquo;vars\u0026rdquo; argument must be a map. Within the template file, each of the keys in the map is available as a variable for interpolation. The template may also use any other function available in the Terraform language, except that recursive calls to templatefile are not permitted. Variable names must each start with a letter, followed by zero or more letters, digits, or underscores.\nAnd our new code looks like this:\nresource \u0026#34;grafana_dashboard\u0026#34; \u0026#34;metrics\u0026#34; { config_json = templatefile(\u0026#34;${path.module}/grafana_dashboard.json\u0026#34;, { title = var.monitoring_title monitoring_datasource_name = var.monitoring_datasource_name }) folder = var.monitoring_folder } Thanks to the new templatefile function, we can get rid of the template_file data source.\nThis means at this point we no longer rely on the hashicorp/template provider and we can apply our infrastructure changes.\nAt this point you can apply the infrastructural changes, but it still might not work and throw the error and this is because if the infrastructure has already been initialized and applied previously we have a record of the deprecated provider stored in the lock file.\nThe template provider is still in the .terraform.lock.hcl file # If you run terraform init and still see that the template provider is being installed:\nInitializing modules... Initializing the backend... Initializing provider plugins... - Reusing previous version of grafana/grafana from the dependency lock file ... - Reusing previous version of hashicorp/template from the dependency lock file ... - Using previously-installed hashicorp/template v2.2.0 Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Then check the Dependency Lock File, terraform.hcl.lock file. If you still see it there:\nprovider \u0026#34;registry.terraform.io/hashicorp/template\u0026#34; { version = \u0026#34;2.2.0\u0026#34; hashes = [ \u0026#34;h1:0wlehNaxBX7GJQnPfQwTNvvAf38Jm0Nv7ssKGMaG6Og=\u0026#34;, \u0026#34;zh:01702196f0a0492ec07917db7aaa595843d8f171dc195f4c988d2ffca2a06386\u0026#34;, \u0026#34;zh:09aae3da826ba3d7df69efeb25d146a1de0d03e951d35019a0f80e4f58c89b53\u0026#34;, \u0026#34;zh:09ba83c0625b6fe0a954da6fbd0c355ac0b7f07f86c91a2a97849140fea49603\u0026#34;, \u0026#34;zh:0e3a6c8e16f17f19010accd0844187d524580d9fdb0731f675ffcf4afba03d16\u0026#34;, \u0026#34;zh:45f2c594b6f2f34ea663704cc72048b212fe7d16fb4cfd959365fa997228a776\u0026#34;, \u0026#34;zh:77ea3e5a0446784d77114b5e851c970a3dde1e08fa6de38210b8385d7605d451\u0026#34;, \u0026#34;zh:8a154388f3708e3df5a69122a23bdfaf760a523788a5081976b3d5616f7d30ae\u0026#34;, \u0026#34;zh:992843002f2db5a11e626b3fc23dc0c87ad3729b3b3cff08e32ffb3df97edbde\u0026#34;, \u0026#34;zh:ad906f4cebd3ec5e43d5cd6dc8f4c5c9cc3b33d2243c89c5fc18f97f7277b51d\u0026#34;, \u0026#34;zh:c979425ddb256511137ecd093e23283234da0154b7fa8b21c2687182d9aea8b2\u0026#34;, ] } Check who is requiring the provider (maybe it\u0026rsquo;s still being used in the code elsewhere). This can be done by running the terraform providers command, which:\nThe terraform providers command shows information about the provider requirements of the configuration in the current working directory, as an aid to understanding where each requirement was detected from.\nProviders required by configuration: . ├── provider[registry.terraform.io/grafana/grafana] ├── ... ├── ... ├── ... ├── module.grafana │ └── provider[registry.terraform.io/grafana/grafana] └── module.module ├── ... Providers required by state: provider[registry.terraform.io/hashicorp/template] provider[registry.terraform.io/grafana/grafana] In this case we can see that the template provider is required by the state.\nIn order to get rid of this dependency, make sure you update Terraform to any versions greater than v.1.1.3. and this is because they fixed the following issue: https://github.com/hashicorp/terraform/pull/30192 in version 1.1.3.\nIn order to update the lock file and remove the entry for the deprecated template provider, we run terraform init.\nThis is because Terraform relies on two sources for determining the truth: the configuration itself and the state. If you remove the dependency on a particular provider from both your configuration and the state then running terraform init will remove any existing lock file entry for that provider.\nAnd let\u0026rsquo;s look at the output:\nInitializing modules... Initializing the backend... Successfully configured the backend \u0026#34;azurerm\u0026#34;! Terraform will automatically use this backend unless the backend configuration changes. Initializing provider plugins... - Reusing previous version of grafana/grafana from the dependency lock file - Using previously-installed grafana/grafana v1.17.0 Terraform has made some changes to the provider dependency selections recorded in the .terraform.lock.hcl file. Review those changes and commit them to your version control system if they represent changes you intended to make. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. And we see the template provider is no longer there.\nNow you can safely commit the freshly updated Dependency Lock File.\n","date":"5 August 2022","externalUrl":null,"permalink":"/posts/terraform-alternative-to-deprecated-template-prov/","section":"Posts","summary":"We ran into an issue while applying our Terraform infrastructure on a M1 Mac where we were making use of the Terraform Provider Template.","title":"Terraform: Alternative to the Template provider on Apple M1 MBP","type":"posts"},{"content":"This article applies to: Helm v3.8.0\nHelm helps you manage Kubernetes applications — Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. More details on Helm and the commands can be found in the official documentation.\nAssuming you use Helm to handle your releases, you might end up in a case where the release will be stuck in a pending state and all subsequent releases will keep failing.\nThis could happen if:\nyou run the upgrade command from the cli and accidentally (or not) interrupt it or you have two deploys running at the same time (maybe in Github Actions, for example) Basically, any interruption that occurred during your install/upgrade process could lead you to a state where you cannot install another release anymore.\nIn the release logs the failing upgrade will show an error similar to the following:\nError: UPGRADE FAILED: release \u0026lt;name\u0026gt; failed, and has been rolled back due to atomic being set: timed out waiting for the condition Error: Error: The process \u0026#39;/usr/bin/helm3\u0026#39; failed with exit code 1 Error: The process \u0026#39;/usr/bin/helm3\u0026#39; failed with exit code 1 And the status will be stuck in: PENDING_INSTALL or PENDING_UPGRADE depending on the command you were running.\nBecause of this pending state when you run the command to list all release it will return empty:\n\u0026gt; helm list --all NAME\tNAMESPACE\tREVISION\tUPDATED\tSTATUS\tCHART\tAPP VERSION So what can we do now? In this article we will look over the two options described below. Keep in mind that based on your setup it could be another issue, but I\u0026rsquo;m hoping that these two pointers will give you a place to start.\nRoll back to the previous working version using the helm rollback command. Delete the helm secret associated with the release and re-run the upgrade command. So let\u0026rsquo;s look at each option in detail.\nFirst option: Roll back to the previous working version using the helm rollback command # From the official documentation:\nThis command rolls back a release to a previous revision. The first argument of the rollback command is the name of a release, and the second is a revision (version) number. If this argument is omitted, it will roll back to the previous release.\nSo, in this case, let\u0026rsquo;s get the history of the releases:\nhelm history \u0026lt;releasename\u0026gt; -n \u0026lt;namespace\u0026gt; In the output you will notice the STATUS of your release with: pending-upgrade:\nREVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Wed May 25 11:45:40 2022 DEPLOYED api-0.1.0 1.16.0 Upgrade complete 2 Mon May 30 14:32:46 2022 PENDING_UPGRADE api-0.1.0 1.16.0 Preparing upgrade Now let\u0026rsquo;s perform the rollback by running the following command:\nhelm rollback \u0026lt;release\u0026gt; \u0026lt;revision\u0026gt; --namespace \u0026lt;namespace\u0026gt; So in our case we run:\n\u0026gt; helm rollback api 1 --namespace api Rollback was a success. After we get confirmation that the rollback was successful, we run the command to get the history again.\nWe now see we have two releases and that our rollback was successful having the STATUS is: deployed\n\u0026gt; helm history api -n api REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Wed May 25 11:45:40 2022 SUPERSEEDED api-0.1.0 1.16.0 Upgrade complete 2 Mon May 30 14:32:46 2022 SUPERSEEDED api-0.1.0 1.16.0 Preparing upgrade 3 Mon May 30 14:45:46 2022 DEPLOYED api-0.1.0 1.16.0 Rollback to 1 So what if the solution above didn\u0026rsquo;t work?\nSecond option: Delete the helm secret associated with the release and re-run the upgrade command # First we get all the secrets for the namespace by running:\nkubectl get secrets -n \u0026lt;namespace\u0026gt; In the output you will notice a list of secrets in the following format:\nNAME TYPE DATA AGE api Opaque 21 473d sh.helm.release.v1.api.v648 helm.sh/release.v1 1 6d5h sh.helm.release.v1.api.v649 helm.sh/release.v1 1 5d1h sh.helm.release.v1.api.v650 helm.sh/release.v1 1 57m So what\u0026rsquo;s in a secret?\nHelm3 makes use of the Kubernetes Secrets object to store any information regarding a release. These secrets are basically used by Helm to store and read it\u0026rsquo;s state every time we run: helm list, helm history or helm upgrade in our case.\nThe naming of the secrets is unique per namespace. The format follows the following convention: sh.helm.release.v1.\u0026lt;release_name\u0026gt;.\u0026lt;release_version\u0026gt;.\nThere is a max of 10 secrets that are stored by default, but you can modify this by setting the --history-max flag in your helm upgrade command.\n\u0026ndash;history-max int limit the maximum number of revisions saved per release. Use 0 for no limit (default 10)\nNow that we know what these secrets are used for, let\u0026rsquo;s delete the helm secret associated with the pending release by running the following command:\nkubectl delete secret sh.helm.release.v1.\u0026lt;release_name\u0026gt;.v\u0026lt;release_version\u0026gt; -n \u0026lt;namespace\u0026gt; Finally, we re-run the helm upgrade command (either from command line or from your deployment workflow), which, if all was good so far, should succeed.\nThere is an open issue with Helm so hopefully these workaround won\u0026rsquo;t be needed anymore. But it\u0026rsquo;s open since 2018.\nOf course there could be other cases or issues, but I hope this is a nice place to start. If you ran into something similar I would love to read your input in what the issue was and how you solved it especially since I didn\u0026rsquo;t find the error message to be intuitive.\nThank you for reading!\n","date":"30 May 2022","externalUrl":null,"permalink":"/posts/k8s-fix-helm-release-failing-with-an-upgrade/","section":"Posts","summary":"This article applies to: Helm v3.","title":"K8s: Fix Helm release failing with an upgrade still in progress","type":"posts"},{"content":"This article applies to Terraform v1.1.4\nWhen using Terraform to manage your infrastructure you will end up in the situation when you want to remove some resources.\nYou can do this in several ways, but most of the time you can also just remove the Terraform configuration by commenting it out the code, or removing the calling of the module, run terraform apply and it will get rid of the resources.\nInfrastructure Setup\nAssuming an infra with the following setup:\n- envs |__ staging |_____ main.tf ... |__ prod |_____ main.tf ... - modules |__ awsresources |_____ s3.tf ... Where main.tf file calls the module responsible for creating the AWS resources we need:\nmodule \u0026#34;aws\u0026#34; { source = \u0026#34;../../modules/awsresources\u0026#34; bucket_name = \u0026#34;my-bucket\u0026#34; fqdn = \u0026#34;my-bucket\u0026#34; deployer_arn = \u0026#34;***\u0026#34; tags = { ... } } And a simple S3 bucket configured as follows:\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;name\u0026#34; { bucket = var.bucket_name acl = \u0026#34;private\u0026#34; policy = data.aws_team_policy_document.bucket_policy.json website { } force_destroy = false tags = var.tags } We want to destroy these resources, specifically the S3 bucket itself.\nRemoval of resources\nIf you run terraform plan it will mark it nicely as to be destroyed:\n# module.module_name.aws_s3_bucket.name will be destroyed # (because aws_s3_bucket.static_site is not in configuration) - resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;static_site\u0026#34; { - acl = \u0026#34;private\u0026#34; -\u0026gt; null - arn = \u0026#34;***\u0026#34; -\u0026gt; null - bucket = \u0026#34;***\u0026#34; -\u0026gt; null - bucket_domain_name = \u0026#34;***\u0026#34; -\u0026gt; null - bucket_regional_domain_name = \u0026#34;***\u0026#34; -\u0026gt; null - force_destroy = false -\u0026gt; null - hosted_zone_id = \u0026#34;***\u0026#34; -\u0026gt; null - id = \u0026#34;***\u0026#34; -\u0026gt; null - policy = jsonencode( ... But running the terraform apply on the same plan and on a non-empty S3 bucket will result in the following ERROR:\n╷ │ Error: error deleting S3 Bucket (***): BucketNotEmpty: The bucket you tried to delete is not empty │ status code: 409, request id: ***, host id: *** │ Let\u0026rsquo;s check the S3 bucket configuration again\nChecking the configuration, we see that we set the flag force_destroy = false in our case. This is actually a very good check in place to have because it protects you from accidental data loss.\nFrom the Terraform aws provider documentation:\nforce_destroy - (Optional, Default:false) A boolean that indicates all objects (including any locked objects) should be deleted from the bucket so that the bucket can be destroyed without error. These objects are not recoverable.\nIn our case we have to set force_destroy = true to allow the bucket to be deleted.\nPay attention: You must apply this change so state is updated first, before running the destroy command.\nSo let\u0026rsquo;s change the value to true and apply it on our resource:\n# module.suspended_workspace.aws_s3_bucket.static_site will be updated in-place ~ resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;name\u0026#34; { ~ force_destroy = false -\u0026gt; true id = \u0026#34;***\u0026#34; tags = { ... } # (12 unchanged attributes hidden) # (2 unchanged blocks hidden) } Apply complete! Resources: 0 added, 1 changed, 0 destroyed. After the setting has been applied successfully, we can start deleting the resources by:\nremoving the module from being called, or commenting out the resource and run a terraform apply again to confirm the destruction of the resources.\nAlternatively, you can also run: terraform plan -destroy -target=aws_s3_bucket.name\nThis time the S3 bucket is deleted successfully.\nWhat to do if you noticed the error after some resources have already been destroyed?\nSo let\u0026rsquo;s say you already started to apply the destruction of the resources and some are successfully destroyed and some are not, including our S3 bucket. What can you do at this point?\nOption 1 You can: Use the -target=resource like below to target the S3 bucket changes only and work only with that resource:\nterraform plan -target=module.mymodule.aws_s3_bucket.name terraform apply -target=module.mymodule.aws_s3_bucket.name or\nterraform plan -target=aws_s3_bucket.name terraform apply -target=aws_s3_bucket.name As a note, you can add multiple resources in any of the commands if you have multiple S3 buckets that need to be deleted.\nOR\nOption 2 You can:\nRe-apply the configuration essentially re-creating the missing resources Setting the force_destroy flag Run terraform apply again to destroy the resources This of course depends on the level of complexity of your infrastructure, in some cases rendering it difficult to do.\nAnd there you go. Hope you find it useful!\n","date":"25 May 2022","externalUrl":null,"permalink":"/posts/terraform-deletion-of-non-empty-s3-bucket/","section":"Posts","summary":"This article applies to Terraform v1.","title":"Terraform: Handling the deletion of a non-empty AWS S3 Bucket","type":"posts"},{"content":"This article applies for Kubernetes v1.15 and above.\nKubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\nIt groups containers that make up an application into logical units for easy management and discovery. But what if something happens to the container? In this case, you might need a quick and easy way to restart it.\nKubernetes Pods usually run until there is a new deployment that will replace them. Therefore, there is no straightforward way to restart a single pod.\nWhat happens when one container fails is that, instead of restarting it, it will be replaced.\nRestarting Pods Options # There are a few available options that we will cover in this article:\nScaling down the number of replicas indicating how many Pods it should be maintaining in the ReplicaSet, effectively removing pods, then scaling back up Causes downtime\nDeleting a single pod, forcing K8s to recreate it Might cause downtime\nStarting a rollout (rolling restart method) No downtime\nSo let\u0026rsquo;s look at each option in a bit of details and keep in mind that each option could work for you depending on your needs. Some questions to ask: is it a live environment? is it a new setup? can you afford and outage on the app?\n1. Changing Replicas # An option for restarting the pods is to effectively \u0026ldquo;shut them off\u0026rdquo; by scaling the number of deployment replicas first to zero:\nkubectl scale deployment \u0026lt;name\u0026gt; --replicas=0 In this case, K8s will remove all the replicas that are no longer required.\nAnd then, scaling them back up to the desired number.\nkubectl scale deployment \u0026lt;name\u0026gt; --replicas=\u0026lt;desired_number\u0026gt; This will stop and terminate all current pods and will schedule new pods in their place.\nBecause we are \u0026ldquo;shutting down\u0026rdquo; pods, this option will cause downtime since there will be no container available. So if you\u0026rsquo;re running on a production system the rolling restart method would be the better approach.\nThe names of the new scheduled pods will be different from the previous ones.\nRun the following command to get the new names of the pods:\nkubectl get pods -n \u0026lt;namespace\u0026gt; 2. Deleting a Pod # First we get all the pods in a namespace by running the following command:\nkubectl get pods -n \u0026lt;namespace\u0026gt; Then we delete a single pod by running the following command:\nkubectl delete pod \u0026lt;pod_name\u0026gt; -n \u0026lt;namespace\u0026gt; K8s will note the change and the state difference and will schedule new pods until the desired state is achieved.\n3. Rolling Restart # From version 1.15 K8s now allows you to execute a rolling restart of your deployment. Note: Not only the kubectl versions needs to be updated, but make sure the cluster is running on this version as well.\nRolling restart is used to restart all the pods from a deployment in sequence by running the following command:\nkubectl rollout restart deployment -n \u0026lt;yournamespace\u0026gt; After running this command, K8s proceeds to shut down and restart each pod in the deployment one by one.\nBecause this is done sequentially, there is always some pod running meaning the application itself will still be available, effectively allowing for zero downtime.\nThank you for reading and I hope this helps someone!\n","date":"25 May 2022","externalUrl":null,"permalink":"/posts/k8s-how-to-restart-pods/","section":"Posts","summary":"This article applies for Kubernetes v1.","title":"K8s: How to restart Kubernetes Pods","type":"posts"},{"content":"","date":"7 December 2021","externalUrl":null,"permalink":"/tags/kong/","section":"Tags","summary":"","title":"Kong","type":"tags"},{"content":"In our current environment we have Kong as our Ingress controller in front of our applications.\nWe are also using the Kong CORS Plugin to enable browsers to make cross-origin requests to our application\u0026rsquo;s backend. The CORS plugin lets you configure the API gateway behavior to support Cross-Origin Resource Sharing (CORS). If you want to dig deeper into what CORS is, please check the CORS glossary link.\nWe also make use of helm charts to handle our deployment.\nThe issue We had it set up to allow all origins and wanted to add our domains only. Adding the list of domains to the Kong CORS Plugin origins parameter resulted in \u0026ldquo;Access-Control-Allow-Origin header missing\u0026rdquo; error and CORS blocking the requests.\nThe initial setup of our CORS plugin\nIn order to get the plugin to work the first thing we needed to do is to:\nEnable the plugin on a service # apiVersion: configuration.konghq.com/v1 kind: KongPlugin metadata: name: service-cors-plugin config: origins: - \u0026#39;*\u0026#39; credentials: true max_age: 3600 preflight_continue: false plugin: cors Apart from enabling it, we have a few other configurations set:\ncredentials - required Documentation:\nFlag to determine whether the Access-Control-Allow-Credentials header should be sent with true as the value.\nWhat this means: By default, CORS does not include cookies on cross-origin requests. If you need the requests to transfer cookies (or other user credentials) then it needs to be enabled. This entails that the server will allow cookies to be included on cross-origin requests.\nFor more details on what the Access-Control-Allow-Credentials header does, please check the MDN Web Docs.\norigins - optional Documentation:\nList of allowed domains for the Access-Control-Allow-Origin header.\nWhat this means: You can whitelist every domain that\u0026rsquo;s allowed to call the API or allow everyone using the wildcard. The latter was our initial setup and we\u0026rsquo;ll come back to this.\npreflight_continue - required Documentation:\nA boolean value that instructs the plugin to proxy the OPTIONS preflight request to the Upstream service.\nWhat this means: This flag decides if the preflight request should be handled by the upstream service (i.e. your API) or if the request is handled by the Ingress itself.\nThe purpose of a CORS preflight request is to check whether the CORS protocol is understood by the server for specific methods and headers. The preflight request is an OPTIONS request, using three HTTP request headers: Access-Control-Request-Method, Access-Control-Request-Headers, and the Origin header. If the server allows it, then it will respond to the preflight request with another request which contains the Access-Control-Allow-Methods response header, which lists the desired method (PUT, DELETE, etc).\nWe set it to false meaning we are letting Kong handle the preflight requests instead of passing them to the upstream service.\nFor more details on what the preflight request is, please check the MDN Web Docs.\nmax_age - optional Documentation:\nThe preflight response can be optionally cached for the requests created in the same URL using Access-Control-Max-Age header.\nWe set it to: 3600 seconds.\nFor the full list of configurations available, refer to the CORS plugin documentation.\nAfter the plugin is configured, then:\nApply the Kong CORS plugin to the Ingress # apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kong-service labels: \u0026lt;labels-omitted\u0026gt; annotations: \u0026lt;either add it here or in the values file depending on what you use\u0026gt; spec: rules: - http: paths: - path: /path-to-your-service backend: serviceName: service-name servicePort: \u0026lt;service-port\u0026gt; and\nReference the annotation to the CORS plugin in environment values file # ingress: annotations: kubernetes.io/ingress.class: \u0026#34;kong\u0026#34; konghq.com/plugins: \u0026#34;service-cors-plugin\u0026#34; As I mentioned we use helm to handle our deploys so we have follow the basic structure (aside from Ingress and the specific project structure).\na deployment.yaml: A basic manifest for creating a Kubernetes deployment a service.yaml: A basic manifest for creating a service endpoint for your deployment a _helpers.tpl: A place to put template helpers that you can re-use throughout the chart a values.yaml file that contains the default values for a chart. These values may be overridden by users during helm install or helm upgrade. For more details on what helm is, charts and how to use them, please take a look over the Helm Docs. They are pretty well documented and can give you a good starting point if you are unfamiliar with this topic.\nBut \u0026hellip;\nAllowing all origins is not a best practice unless you are implementing a public API so we wanted to limit the whitelisted domains.\nCheck here to see an example of how the headers can be exploited if you use a wildcard.\nAttempts\nFailed attempt 1 We tried adding the domain using the ASP.Net Core flavor: https://*.domain.com, but it didn\u0026rsquo;t work as the wildcards cannot be used within any other value. For example, the following header is not valid:\nAccess-Control-Allow-Origin: https://*.domain.com In the plugin documentation it says that for the origins param:\nThe accepted values can either be flat strings or PCRE regexes.\nFailed attempt 2 We tried adding a PCRE compliant regex enclosed in double quotes, but helm deploy failed as it rejected the format.\nAdding all the domains with subdomains to a list was not feasible because it\u0026rsquo;s something we register dynamically.\nThe solution\nSo what worked for us was: adding the PCRE regex to whitelist all our subdomains, but enclosed in single quotes. This allowed helm deploy to succeed.\nTo validate your regex you can use the awesome regex101.\nJust make sure you select the PCRE engine flavor.\napiVersion: configuration.konghq.com/v1 kind: KongPlugin metadata: name: service-cors-plugin config: origins: - \u0026#39;https:\\/\\/[\\w-_]+\\.domain1\\.com\u0026#39; - \u0026#39;https://domain2.dev\u0026#39; - \u0026#39;https:\\/\\/[\\w-_]+\\.domain2\\.dev\u0026#39; credentials: true max_age: 3600 preflight_continue: false plugin: cors Alternatively, if you use go templating and use a _helpers.tpl file then you can also define it as below:\napiVersion: configuration.konghq.com/v1 kind: KongPlugin metadata: name: {{ include \u0026#34;service.name\u0026#34; . }}-cors-plugin config: origins: {{- range .Values.CorsAllowedOrigins }} - {{ . | squote }} {{- end }} credentials: true max_age: 3600 preflight_continue: false plugin: cors If you are unfamiliar with what the values.yaml file actually is, it\u0026rsquo;s just a place to put template helpers that you can re-use throughout the chart.\nFor more details on how to read this, please check the Helm Docs on the Values files.\nAnd in the values.yaml file just add your domains or your regex as:\nCorsAllowedOrigins: - \u0026#39;https:\\/\\/[\\w-_]+\\.domain1\\.com\u0026#39; - \u0026#39;https://domain2.dev\u0026#39; - \u0026#39;https:\\/\\/[\\w-_]+\\.domain2\\.dev\u0026#39; And that was that! :)\nI wrote this blog post because all the examples from the internet were using the '*' format for origins and we wanted to allow the specific domains using a regex.\nHope it helps someone else!\n","date":"7 December 2021","externalUrl":null,"permalink":"/posts/kong-pluging-cors-policy/","section":"Posts","summary":"In our current environment we have Kong as our Ingress controller in front of our applications.","title":"KongPlugin CORS: fixing Access-Control-Allow-Origin header missing error","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]